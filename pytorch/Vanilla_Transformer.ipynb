{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuCVUQjkPWsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext seaborn "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZDZqTsLPfCx",
        "colab_type": "code",
        "outputId": "d948519c-af2d-43a4-a295-cb66b69f298a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Use SpaCy to download IWSLT German-English Translation data\n",
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de\n",
        "!pip install ipdb\n",
        "\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.17.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.3.1)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.9.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 624kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-cp36-none-any.whl size=11073065 sha256=9c3cf30847831e6dace7ad77411ed6d09f93d552a23e1a9c3dbdf4d14f89628e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z55zvoyp/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n",
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/df/78/3d0d7253dc85549db182cbe4b43b30c506c84008fcd39898122c9b6306a9/ipdb-0.12.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from ipdb) (41.6.0)\n",
            "Requirement already satisfied: ipython>=5.1.0 in /usr/local/lib/python3.6/dist-packages (from ipdb) (5.5.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.7.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=5.1.0->ipdb) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (1.12.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython>=5.1.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.1.0->ipdb) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.1.0->ipdb) (0.1.7)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.12.2-cp36-none-any.whl size=9171 sha256=4aeebb2c73be553b2930a7b97cd9474904e453b750d7e3bf65b8054ac8a8e44e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/00/07/c906eaf1b90367fbb81bd840e56bf8859dbd3efe3838c0b4ba\n",
            "Successfully built ipdb\n",
            "Installing collected packages: ipdb\n",
            "Successfully installed ipdb-0.12.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFW3aZxQlcE_",
        "colab_type": "code",
        "outputId": "62d56658-3a79-45b1-efe1-993e403737cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pORknqfOPq2E",
        "colab_type": "text"
      },
      "source": [
        "**Encoder Decoder Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0EZzWTlPo2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxVvdiLqQDRa",
        "colab_type": "text"
      },
      "source": [
        "**Feed Forward and Softmax for final word prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9X8jGSmQCUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CKovdezRLT6",
        "colab_type": "text"
      },
      "source": [
        "**Creating the stacked (self-attention + FC) layers for the encoder or decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Cb9rkTSRMC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcXqoANRiz4",
        "colab_type": "text"
      },
      "source": [
        "**Encoder Wrapper which has multiple layers (N=6), each with \"self-attention -> LayerNorm -> FC -> LayerNorm\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr1WKUFiRhYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J5NErGUUO5H",
        "colab_type": "text"
      },
      "source": [
        "**LayerNorm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9RlBlRiUPLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lta0gl2lYNwi",
        "colab_type": "text"
      },
      "source": [
        "**Apply dropout to output of sublayer before adding the residual input and normalising the sum**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFzgI6V0YODZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lGwAzwhZhlD",
        "colab_type": "text"
      },
      "source": [
        "**Single Encoder layer with a self-attention and FC layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFAFMQvJZhv1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MouZRDEdUWj",
        "colab_type": "text"
      },
      "source": [
        "**Encoder Wrapper which has multiple layers (N=6), each with \"masked self-attention for encoder output -> LayerNorm -> self-attention-> LayerNorm-> FC -> LayerNorm\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pg4oLAodUqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUm8HoV1eV6A",
        "colab_type": "text"
      },
      "source": [
        "**Single Decoder layer with masked self-attention, self-attention and FC layer** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La5swI0geWMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGc1DJuKipZA",
        "colab_type": "text"
      },
      "source": [
        "**Subsequent Mask that ensures that self-attention only attends to positions before the token about to be predicted**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apAPU2NDip1K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGRBx8ws_R51",
        "colab_type": "text"
      },
      "source": [
        "**Attention function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i6baSN6_SKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ru-2r9QS-LJ",
        "colab_type": "text"
      },
      "source": [
        "**Multi-head Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPYSEX0xTAXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoG7Kw_7ZWiA",
        "colab_type": "text"
      },
      "source": [
        "**Point-wise Feed-Forward Layer (FC Layer)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk76TejzZW-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58DJm0HKwOCt",
        "colab_type": "text"
      },
      "source": [
        "**Embedding Layer used by Encoder and Decoder stacks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5LhRvkOwOeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T868vE6ghIov",
        "colab_type": "text"
      },
      "source": [
        "**Positional Encoding for injecting order information of the input and output sequences into the encoder and decoder, respectively**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lZWCoDbhI7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH4l41RpkpPt",
        "colab_type": "text"
      },
      "source": [
        "**Full Transformer Model Constructor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ivTKKMkpnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxUQL_h5lkpO",
        "colab_type": "text"
      },
      "source": [
        "**Creating a Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itYkCg51lkz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Small example model.\n",
        "# tmp_model = make_model(10, 10, 2)\n",
        "# None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oywxev0nsoh",
        "colab_type": "text"
      },
      "source": [
        "**Batch Class for holding batches of masked input/output sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBkyL2Yyns_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krAX4LjWqdcP",
        "colab_type": "text"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCTf51nqdmu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        batch.src = batch.src.to(DEVICE)\n",
        "        batch.trg = batch.trg.to(DEVICE)\n",
        "        batch.src_mask = batch.src_mask.to(DEVICE)\n",
        "        batch.trg_mask = batch.trg_mask.to(DEVICE)\n",
        "        batch.trg_y = batch.trg_y.to(DEVICE)\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnOk9suBq1N3",
        "colab_type": "text"
      },
      "source": [
        "**Batching the Data using torch text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtH9EzGcq1X1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBreV-ClrKyJ",
        "colab_type": "text"
      },
      "source": [
        "**Specialised Transformer Optimiser**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-Ivpw83rLHT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXBDi640tg0s",
        "colab_type": "text"
      },
      "source": [
        "**Regularisation using Label Smoothing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVtByx3ithJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.sum() and len(mask) > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8RvEbRwuP3h",
        "colab_type": "text"
      },
      "source": [
        "**Synthetic Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5fm-LGBuQEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_gen(V, batch, nbatches):\n",
        "    \"Generate random data for a src-tgt copy task.\"\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        data[:, 0] = 1\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        yield Batch(src, tgt, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3_L9xIcuZdf",
        "colab_type": "text"
      },
      "source": [
        "**Simple Loss Computation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43aFYYBkuZp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.data * norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mRCW2khuoNi",
        "colab_type": "text"
      },
      "source": [
        "**Training Using Toy Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2INutKouobj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the simple copy task.\n",
        "# V = 11\n",
        "# criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "# model = make_model(V, V, N=2)\n",
        "# model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
        "#         torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "# for epoch in range(10):\n",
        "#     model.train()\n",
        "#     run_epoch(data_gen(V, 30, 20), model, \n",
        "#               SimpleLossCompute(model.generator, criterion, model_opt))\n",
        "#     model.eval()\n",
        "#     print(run_epoch(data_gen(V, 30, 5), model, \n",
        "#                     SimpleLossCompute(model.generator, criterion, None)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcxmYRrA2ZMS",
        "colab_type": "text"
      },
      "source": [
        "**Loading Real World Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p91yY4Ak2ZZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "ab9aaafe-ec95-4a73-aa3a-41f155b16284"
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets\n",
        "\n",
        "if True:\n",
        "    import spacy\n",
        "    spacy_de = spacy.load('de')\n",
        "    spacy_en = spacy.load('en')\n",
        "\n",
        "    def tokenize_de(text):\n",
        "        return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "    def tokenize_en(text):\n",
        "        return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                     eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = datasets.IWSLT.splits(\n",
        "        exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "            len(vars(x)['trg']) <= MAX_LEN)\n",
        "    MIN_FREQ = 2\n",
        "    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rde-en.tgz:   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading de-en.tgz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "de-en.tgz: 100%|██████████| 24.2M/24.2M [00:00<00:00, 36.2MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2014.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2013.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.dev2010.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2010.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2011.de-en.en.xml\n",
            ".data/iwslt/de-en/IWSLT16.TEDX.dev2012.de-en.de.xml\n",
            ".data/iwslt/de-en/IWSLT16.TED.tst2012.de-en.en.xml\n",
            ".data/iwslt/de-en/train.tags.de-en.de\n",
            ".data/iwslt/de-en/train.tags.de-en.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQOtyJ-Q2tZs",
        "colab_type": "text"
      },
      "source": [
        "**Batch Iterator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXImJEJX2tlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-AIxhyAiUKo",
        "colab_type": "text"
      },
      "source": [
        "**Create model, criterion, optimiser and data iterator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsMF3SLLiR4f",
        "colab_type": "code",
        "outputId": "1e4867f7-a128-4405-db7e-45628f16856a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if True:\n",
        "    pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "    model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "    model.cuda()\n",
        "    # model = model.to(DEVICE)\n",
        "    criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "    criterion.cuda()\n",
        "    # criterion = criterion.to(DEVICE)\n",
        "    BATCH_SIZE = 1000\n",
        "    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "None"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBtLJZfIixuK",
        "colab_type": "text"
      },
      "source": [
        "**Training the system**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM2S5kpkiyEb",
        "colab_type": "code",
        "outputId": "eda34760-2780-4804-f672-8ab5a75e891f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/opennmt-models/iwslt.pt"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-21 21:49:46--  https://s3.amazonaws.com/opennmt-models/iwslt.pt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.104.245\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.104.245|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 467317581 (446M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘iwslt.pt’\n",
            "\n",
            "iwslt.pt            100%[===================>] 445.67M  17.8MB/s    in 17s     \n",
            "\n",
            "2019-11-21 21:50:04 (26.1 MB/s) - ‘iwslt.pt’ saved [467317581/467317581]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-LDrWMPmCA9",
        "colab_type": "code",
        "outputId": "1a5b4c51-bc2a-4967-fca4-eb654573a628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla P100-PCIE-16GB'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu3mGy5nDgCM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "9e425c03-200d-4f8d-8334-ebfebd205783"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78-H737mjFM6",
        "colab_type": "code",
        "outputId": "e96b92d0-533f-4e7e-adab-36cabfeb9891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# if False:\n",
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "for epoch in range(10):\n",
        "    print(\"Starting epoch %s ...\" % (epoch))\n",
        "    model.train()\n",
        "    run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "              model, \n",
        "              SimpleLossCompute(model.generator, criterion, opt=model_opt))\n",
        "    model.eval()\n",
        "    loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                      model, \n",
        "                      SimpleLossCompute(model.generator, criterion, None))\n",
        "    print(loss)\n",
        "    print(\"Ending epoch %s ...\" % (epoch))\n",
        "    torch.save(model.state_dict(), '/content/drive/My Drive/vanilla_transformer_%s.pt' % (epoch))\n",
        "# else:\n",
        "#     model = torch.load(\"iwslt.pt\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 0 ...\n",
            "Epoch Step: 1 Loss: 9.163675 Tokens per Sec: 1044.612305\n",
            "Epoch Step: 51 Loss: 8.582150 Tokens per Sec: 2475.953613\n",
            "Epoch Step: 101 Loss: 7.486285 Tokens per Sec: 2453.150391\n",
            "Epoch Step: 151 Loss: 6.269614 Tokens per Sec: 2447.238281\n",
            "Epoch Step: 201 Loss: 5.578363 Tokens per Sec: 2425.768799\n",
            "Epoch Step: 251 Loss: 5.549174 Tokens per Sec: 2457.658447\n",
            "Epoch Step: 301 Loss: 5.014129 Tokens per Sec: 2388.038330\n",
            "Epoch Step: 351 Loss: 5.415132 Tokens per Sec: 2468.358887\n",
            "Epoch Step: 401 Loss: 4.771523 Tokens per Sec: 2415.973633\n",
            "Epoch Step: 451 Loss: 4.705500 Tokens per Sec: 2484.789795\n",
            "Epoch Step: 501 Loss: 4.820636 Tokens per Sec: 2424.707031\n",
            "Epoch Step: 551 Loss: 4.852473 Tokens per Sec: 2463.845215\n",
            "Epoch Step: 601 Loss: 4.914701 Tokens per Sec: 2440.384277\n",
            "Epoch Step: 651 Loss: 4.497285 Tokens per Sec: 2488.472656\n",
            "Epoch Step: 701 Loss: 4.529559 Tokens per Sec: 2441.735840\n",
            "Epoch Step: 751 Loss: 4.634816 Tokens per Sec: 2406.721191\n",
            "Epoch Step: 801 Loss: 4.689807 Tokens per Sec: 2447.022217\n",
            "Epoch Step: 851 Loss: 4.754286 Tokens per Sec: 2455.738037\n",
            "Epoch Step: 901 Loss: 4.207258 Tokens per Sec: 2443.429688\n",
            "Epoch Step: 951 Loss: 4.796761 Tokens per Sec: 2477.848145\n",
            "Epoch Step: 1001 Loss: 4.733557 Tokens per Sec: 2452.687256\n",
            "Epoch Step: 1051 Loss: 4.657718 Tokens per Sec: 2412.289062\n",
            "Epoch Step: 1101 Loss: 4.564803 Tokens per Sec: 2457.784668\n",
            "Epoch Step: 1151 Loss: 4.619730 Tokens per Sec: 2468.039062\n",
            "Epoch Step: 1201 Loss: 4.009665 Tokens per Sec: 2465.189697\n",
            "Epoch Step: 1251 Loss: 4.090038 Tokens per Sec: 2485.035400\n",
            "Epoch Step: 1301 Loss: 4.607954 Tokens per Sec: 2466.958984\n",
            "Epoch Step: 1351 Loss: 4.469356 Tokens per Sec: 2415.778809\n",
            "Epoch Step: 1401 Loss: 4.514626 Tokens per Sec: 2434.734375\n",
            "Epoch Step: 1451 Loss: 3.623983 Tokens per Sec: 2405.473633\n",
            "Epoch Step: 1501 Loss: 4.139772 Tokens per Sec: 2463.673584\n",
            "Epoch Step: 1551 Loss: 4.390270 Tokens per Sec: 2483.180664\n",
            "Epoch Step: 1601 Loss: 4.255054 Tokens per Sec: 2456.454590\n",
            "Epoch Step: 1651 Loss: 4.319799 Tokens per Sec: 2447.138672\n",
            "Epoch Step: 1701 Loss: 3.839124 Tokens per Sec: 2443.945557\n",
            "Epoch Step: 1751 Loss: 3.953398 Tokens per Sec: 2430.553955\n",
            "Epoch Step: 1801 Loss: 4.222980 Tokens per Sec: 2445.626465\n",
            "Epoch Step: 1851 Loss: 4.330516 Tokens per Sec: 2463.384766\n",
            "Epoch Step: 1901 Loss: 4.602643 Tokens per Sec: 2453.663574\n",
            "Epoch Step: 1951 Loss: 4.334318 Tokens per Sec: 2462.515625\n",
            "Epoch Step: 2001 Loss: 3.957324 Tokens per Sec: 2469.119873\n",
            "Epoch Step: 2051 Loss: 4.317145 Tokens per Sec: 2482.826660\n",
            "Epoch Step: 2101 Loss: 3.762110 Tokens per Sec: 2419.496094\n",
            "Epoch Step: 2151 Loss: 3.748427 Tokens per Sec: 2441.208008\n",
            "Epoch Step: 2201 Loss: 3.780388 Tokens per Sec: 2397.972168\n",
            "Epoch Step: 2251 Loss: 3.656828 Tokens per Sec: 2477.424805\n",
            "Epoch Step: 2301 Loss: 3.546516 Tokens per Sec: 2396.621582\n",
            "Epoch Step: 2351 Loss: 4.483058 Tokens per Sec: 2448.819824\n",
            "Epoch Step: 2401 Loss: 4.074831 Tokens per Sec: 2492.633789\n",
            "Epoch Step: 2451 Loss: 4.496568 Tokens per Sec: 2472.178955\n",
            "Epoch Step: 2501 Loss: 3.789873 Tokens per Sec: 2435.949707\n",
            "Epoch Step: 2551 Loss: 2.806283 Tokens per Sec: 2423.381104\n",
            "Epoch Step: 2601 Loss: 4.413068 Tokens per Sec: 2474.116455\n",
            "Epoch Step: 2651 Loss: 3.575062 Tokens per Sec: 2451.435303\n",
            "Epoch Step: 2701 Loss: 4.199297 Tokens per Sec: 2454.349854\n",
            "Epoch Step: 2751 Loss: 4.285604 Tokens per Sec: 2477.158203\n",
            "Epoch Step: 2801 Loss: 3.210903 Tokens per Sec: 2362.449951\n",
            "Epoch Step: 2851 Loss: 4.001940 Tokens per Sec: 2447.965332\n",
            "Epoch Step: 2901 Loss: 4.024320 Tokens per Sec: 2464.618652\n",
            "Epoch Step: 2951 Loss: 3.927174 Tokens per Sec: 2408.199219\n",
            "Epoch Step: 3001 Loss: 3.762913 Tokens per Sec: 2440.356934\n",
            "Epoch Step: 3051 Loss: 4.196203 Tokens per Sec: 2387.369873\n",
            "Epoch Step: 3101 Loss: 4.081048 Tokens per Sec: 2421.623291\n",
            "Epoch Step: 3151 Loss: 3.994498 Tokens per Sec: 2448.315430\n",
            "Epoch Step: 3201 Loss: 4.093378 Tokens per Sec: 2474.901855\n",
            "Epoch Step: 3251 Loss: 4.136257 Tokens per Sec: 2422.019775\n",
            "Epoch Step: 3301 Loss: 3.917509 Tokens per Sec: 2440.496094\n",
            "Epoch Step: 3351 Loss: 3.679884 Tokens per Sec: 2472.486084\n",
            "Epoch Step: 3401 Loss: 3.150266 Tokens per Sec: 2471.789551\n",
            "Epoch Step: 3451 Loss: 4.118006 Tokens per Sec: 2438.678955\n",
            "Epoch Step: 3501 Loss: 3.856006 Tokens per Sec: 2450.025635\n",
            "Epoch Step: 3551 Loss: 2.663771 Tokens per Sec: 2451.725342\n",
            "Epoch Step: 3601 Loss: 3.913346 Tokens per Sec: 2482.972168\n",
            "Epoch Step: 3651 Loss: 3.680687 Tokens per Sec: 2421.548584\n",
            "Epoch Step: 3701 Loss: 3.967377 Tokens per Sec: 2425.953125\n",
            "Epoch Step: 3751 Loss: 4.094563 Tokens per Sec: 2452.730713\n",
            "Epoch Step: 3801 Loss: 3.529446 Tokens per Sec: 2424.065674\n",
            "Epoch Step: 3851 Loss: 3.976592 Tokens per Sec: 2432.700195\n",
            "Epoch Step: 3901 Loss: 3.960826 Tokens per Sec: 2445.281250\n",
            "Epoch Step: 3951 Loss: 4.132420 Tokens per Sec: 2453.843262\n",
            "Epoch Step: 4001 Loss: 4.197043 Tokens per Sec: 2454.205811\n",
            "Epoch Step: 4051 Loss: 3.783011 Tokens per Sec: 2446.806152\n",
            "Epoch Step: 4101 Loss: 3.235524 Tokens per Sec: 2459.490234\n",
            "Epoch Step: 4151 Loss: 4.006584 Tokens per Sec: 2445.843262\n",
            "Epoch Step: 4201 Loss: 3.528484 Tokens per Sec: 2446.483154\n",
            "Epoch Step: 4251 Loss: 4.497573 Tokens per Sec: 2447.742676\n",
            "Epoch Step: 4301 Loss: 4.481526 Tokens per Sec: 2437.780762\n",
            "Epoch Step: 4351 Loss: 4.145930 Tokens per Sec: 2470.491699\n",
            "Epoch Step: 4401 Loss: 3.801024 Tokens per Sec: 2384.199707\n",
            "Epoch Step: 4451 Loss: 3.767840 Tokens per Sec: 2448.047119\n",
            "Epoch Step: 4501 Loss: 3.713711 Tokens per Sec: 2466.188477\n",
            "Epoch Step: 1 Loss: 3.080745 Tokens per Sec: 2065.765381\n",
            "tensor(3.6756, device='cuda:0')\n",
            "Ending epoch 0 ...\n",
            "Starting epoch 1 ...\n",
            "Epoch Step: 1 Loss: 4.079930 Tokens per Sec: 1154.559082\n",
            "Epoch Step: 51 Loss: 4.093597 Tokens per Sec: 2420.106689\n",
            "Epoch Step: 101 Loss: 3.951268 Tokens per Sec: 2472.018555\n",
            "Epoch Step: 151 Loss: 4.241525 Tokens per Sec: 2426.030029\n",
            "Epoch Step: 201 Loss: 3.988381 Tokens per Sec: 2432.750732\n",
            "Epoch Step: 251 Loss: 4.224665 Tokens per Sec: 2422.203369\n",
            "Epoch Step: 301 Loss: 3.901399 Tokens per Sec: 2440.107178\n",
            "Epoch Step: 351 Loss: 3.709634 Tokens per Sec: 2446.682617\n",
            "Epoch Step: 401 Loss: 4.022240 Tokens per Sec: 2461.072021\n",
            "Epoch Step: 451 Loss: 3.250587 Tokens per Sec: 2409.745850\n",
            "Epoch Step: 501 Loss: 3.637866 Tokens per Sec: 2448.478760\n",
            "Epoch Step: 551 Loss: 3.781638 Tokens per Sec: 2478.783447\n",
            "Epoch Step: 601 Loss: 3.559465 Tokens per Sec: 2434.811768\n",
            "Epoch Step: 651 Loss: 3.973387 Tokens per Sec: 2410.630859\n",
            "Epoch Step: 701 Loss: 3.249788 Tokens per Sec: 2473.149170\n",
            "Epoch Step: 751 Loss: 4.026773 Tokens per Sec: 2496.005371\n",
            "Epoch Step: 801 Loss: 2.899008 Tokens per Sec: 2444.454834\n",
            "Epoch Step: 851 Loss: 4.373556 Tokens per Sec: 2433.078369\n",
            "Epoch Step: 901 Loss: 3.351211 Tokens per Sec: 2464.600098\n",
            "Epoch Step: 951 Loss: 3.456499 Tokens per Sec: 2432.567871\n",
            "Epoch Step: 1001 Loss: 3.265052 Tokens per Sec: 2440.326904\n",
            "Epoch Step: 1051 Loss: 3.906512 Tokens per Sec: 2474.688232\n",
            "Epoch Step: 1101 Loss: 3.284551 Tokens per Sec: 2445.119385\n",
            "Epoch Step: 1151 Loss: 4.006444 Tokens per Sec: 2425.644043\n",
            "Epoch Step: 1201 Loss: 4.190538 Tokens per Sec: 2443.440430\n",
            "Epoch Step: 1251 Loss: 4.011091 Tokens per Sec: 2467.362793\n",
            "Epoch Step: 1301 Loss: 3.738718 Tokens per Sec: 2444.804199\n",
            "Epoch Step: 1351 Loss: 3.494260 Tokens per Sec: 2476.822021\n",
            "Epoch Step: 1401 Loss: 3.215966 Tokens per Sec: 2468.658691\n",
            "Epoch Step: 1451 Loss: 4.167964 Tokens per Sec: 2467.736816\n",
            "Epoch Step: 1501 Loss: 3.718329 Tokens per Sec: 2448.502441\n",
            "Epoch Step: 1551 Loss: 4.141208 Tokens per Sec: 2451.934570\n",
            "Epoch Step: 1601 Loss: 2.950739 Tokens per Sec: 2482.476318\n",
            "Epoch Step: 1651 Loss: 3.902578 Tokens per Sec: 2442.093506\n",
            "Epoch Step: 1701 Loss: 3.247779 Tokens per Sec: 2433.465576\n",
            "Epoch Step: 1751 Loss: 3.597708 Tokens per Sec: 2444.472900\n",
            "Epoch Step: 1801 Loss: 4.040869 Tokens per Sec: 2412.325928\n",
            "Epoch Step: 1851 Loss: 3.900349 Tokens per Sec: 2466.811035\n",
            "Epoch Step: 1901 Loss: 3.297286 Tokens per Sec: 2408.336914\n",
            "Epoch Step: 1951 Loss: 3.196085 Tokens per Sec: 2477.267578\n",
            "Epoch Step: 2001 Loss: 3.750995 Tokens per Sec: 2483.157959\n",
            "Epoch Step: 2051 Loss: 3.712197 Tokens per Sec: 2409.579102\n",
            "Epoch Step: 2101 Loss: 4.275442 Tokens per Sec: 2444.632080\n",
            "Epoch Step: 2151 Loss: 3.401073 Tokens per Sec: 2461.499512\n",
            "Epoch Step: 2201 Loss: 3.890231 Tokens per Sec: 2419.504395\n",
            "Epoch Step: 2251 Loss: 3.125543 Tokens per Sec: 2416.617432\n",
            "Epoch Step: 2301 Loss: 4.416130 Tokens per Sec: 2327.858154\n",
            "Epoch Step: 2351 Loss: 2.947762 Tokens per Sec: 2409.059814\n",
            "Epoch Step: 2401 Loss: 4.155340 Tokens per Sec: 2467.502930\n",
            "Epoch Step: 2451 Loss: 4.289705 Tokens per Sec: 2464.483643\n",
            "Epoch Step: 2501 Loss: 4.502312 Tokens per Sec: 2462.260498\n",
            "Epoch Step: 2551 Loss: 4.288233 Tokens per Sec: 2432.765625\n",
            "Epoch Step: 2601 Loss: 3.476385 Tokens per Sec: 2454.269287\n",
            "Epoch Step: 2651 Loss: 3.567582 Tokens per Sec: 2469.074219\n",
            "Epoch Step: 2701 Loss: 3.431677 Tokens per Sec: 2418.602539\n",
            "Epoch Step: 2751 Loss: 3.633143 Tokens per Sec: 2412.805908\n",
            "Epoch Step: 2801 Loss: 3.509160 Tokens per Sec: 2466.904297\n",
            "Epoch Step: 2851 Loss: 3.560492 Tokens per Sec: 2461.929443\n",
            "Epoch Step: 2901 Loss: 2.745476 Tokens per Sec: 2455.345703\n",
            "Epoch Step: 2951 Loss: 3.566403 Tokens per Sec: 2465.942139\n",
            "Epoch Step: 3001 Loss: 3.128055 Tokens per Sec: 2450.638672\n",
            "Epoch Step: 3051 Loss: 3.068910 Tokens per Sec: 2433.864502\n",
            "Epoch Step: 3101 Loss: 3.747732 Tokens per Sec: 2443.872803\n",
            "Epoch Step: 3151 Loss: 3.026110 Tokens per Sec: 2430.587891\n",
            "Epoch Step: 3201 Loss: 4.074626 Tokens per Sec: 2422.885254\n",
            "Epoch Step: 3251 Loss: 2.840721 Tokens per Sec: 2446.060547\n",
            "Epoch Step: 3301 Loss: 3.703704 Tokens per Sec: 2399.712402\n",
            "Epoch Step: 3351 Loss: 2.494300 Tokens per Sec: 2462.727295\n",
            "Epoch Step: 3401 Loss: 3.083224 Tokens per Sec: 2418.054199\n",
            "Epoch Step: 3451 Loss: 3.334243 Tokens per Sec: 2445.744385\n",
            "Epoch Step: 3501 Loss: 4.006069 Tokens per Sec: 2474.799072\n",
            "Epoch Step: 3551 Loss: 3.565380 Tokens per Sec: 2467.410645\n",
            "Epoch Step: 3601 Loss: 3.772548 Tokens per Sec: 2435.691162\n",
            "Epoch Step: 3651 Loss: 3.358479 Tokens per Sec: 2475.399902\n",
            "Epoch Step: 3701 Loss: 4.375727 Tokens per Sec: 2401.743652\n",
            "Epoch Step: 3751 Loss: 3.672501 Tokens per Sec: 2464.658691\n",
            "Epoch Step: 3801 Loss: 3.777485 Tokens per Sec: 2378.992188\n",
            "Epoch Step: 3851 Loss: 3.988747 Tokens per Sec: 2442.027344\n",
            "Epoch Step: 3901 Loss: 4.070952 Tokens per Sec: 2456.927490\n",
            "Epoch Step: 3951 Loss: 3.907929 Tokens per Sec: 2450.704346\n",
            "Epoch Step: 4001 Loss: 3.634945 Tokens per Sec: 2417.124023\n",
            "Epoch Step: 4051 Loss: 3.686110 Tokens per Sec: 2465.483398\n",
            "Epoch Step: 4101 Loss: 3.463028 Tokens per Sec: 2423.099609\n",
            "Epoch Step: 4151 Loss: 3.797878 Tokens per Sec: 2442.691162\n",
            "Epoch Step: 4201 Loss: 3.550888 Tokens per Sec: 2455.641846\n",
            "Epoch Step: 4251 Loss: 3.616655 Tokens per Sec: 2459.375977\n",
            "Epoch Step: 4301 Loss: 3.902367 Tokens per Sec: 2435.836182\n",
            "Epoch Step: 4351 Loss: 3.955616 Tokens per Sec: 2412.681396\n",
            "Epoch Step: 4401 Loss: 3.517141 Tokens per Sec: 2459.388672\n",
            "Epoch Step: 4451 Loss: 4.157927 Tokens per Sec: 2456.518311\n",
            "Epoch Step: 4501 Loss: 3.732580 Tokens per Sec: 2458.860107\n",
            "Epoch Step: 1 Loss: 2.771802 Tokens per Sec: 2076.481689\n",
            "tensor(3.4098, device='cuda:0')\n",
            "Ending epoch 1 ...\n",
            "Starting epoch 2 ...\n",
            "Epoch Step: 1 Loss: 3.759915 Tokens per Sec: 1274.603394\n",
            "Epoch Step: 51 Loss: 3.639612 Tokens per Sec: 2436.541992\n",
            "Epoch Step: 101 Loss: 3.661853 Tokens per Sec: 2414.414551\n",
            "Epoch Step: 151 Loss: 3.250372 Tokens per Sec: 2474.758545\n",
            "Epoch Step: 201 Loss: 3.533489 Tokens per Sec: 2442.056885\n",
            "Epoch Step: 251 Loss: 3.031923 Tokens per Sec: 2471.536377\n",
            "Epoch Step: 301 Loss: 3.013582 Tokens per Sec: 2466.550781\n",
            "Epoch Step: 351 Loss: 3.567166 Tokens per Sec: 2470.156250\n",
            "Epoch Step: 401 Loss: 3.879483 Tokens per Sec: 2414.483643\n",
            "Epoch Step: 451 Loss: 3.136062 Tokens per Sec: 2448.614258\n",
            "Epoch Step: 501 Loss: 3.236630 Tokens per Sec: 2476.494629\n",
            "Epoch Step: 551 Loss: 3.066963 Tokens per Sec: 2421.144043\n",
            "Epoch Step: 601 Loss: 3.778641 Tokens per Sec: 2449.352539\n",
            "Epoch Step: 651 Loss: 3.807162 Tokens per Sec: 2467.863525\n",
            "Epoch Step: 701 Loss: 4.187279 Tokens per Sec: 2479.634521\n",
            "Epoch Step: 751 Loss: 3.547875 Tokens per Sec: 2390.345703\n",
            "Epoch Step: 801 Loss: 3.078957 Tokens per Sec: 2395.427490\n",
            "Epoch Step: 851 Loss: 3.486690 Tokens per Sec: 2495.746094\n",
            "Epoch Step: 901 Loss: 3.032898 Tokens per Sec: 2428.579102\n",
            "Epoch Step: 951 Loss: 3.751523 Tokens per Sec: 2415.106445\n",
            "Epoch Step: 1001 Loss: 3.478189 Tokens per Sec: 2421.169189\n",
            "Epoch Step: 1051 Loss: 3.925070 Tokens per Sec: 2465.783447\n",
            "Epoch Step: 1101 Loss: 3.228505 Tokens per Sec: 2388.690186\n",
            "Epoch Step: 1151 Loss: 3.823193 Tokens per Sec: 2462.293945\n",
            "Epoch Step: 1201 Loss: 4.037474 Tokens per Sec: 2458.297119\n",
            "Epoch Step: 1251 Loss: 3.942413 Tokens per Sec: 2449.626953\n",
            "Epoch Step: 1301 Loss: 3.334988 Tokens per Sec: 2478.938965\n",
            "Epoch Step: 1351 Loss: 3.801683 Tokens per Sec: 2400.167236\n",
            "Epoch Step: 1401 Loss: 4.148669 Tokens per Sec: 2439.500488\n",
            "Epoch Step: 1451 Loss: 4.359079 Tokens per Sec: 2400.386475\n",
            "Epoch Step: 1501 Loss: 3.534837 Tokens per Sec: 2474.797607\n",
            "Epoch Step: 1551 Loss: 3.472311 Tokens per Sec: 2463.737305\n",
            "Epoch Step: 1601 Loss: 4.191110 Tokens per Sec: 2437.846436\n",
            "Epoch Step: 1651 Loss: 3.432249 Tokens per Sec: 2468.663574\n",
            "Epoch Step: 1701 Loss: 3.272848 Tokens per Sec: 2458.387451\n",
            "Epoch Step: 1751 Loss: 3.999813 Tokens per Sec: 2465.069092\n",
            "Epoch Step: 1801 Loss: 3.095672 Tokens per Sec: 2453.184326\n",
            "Epoch Step: 1851 Loss: 3.126051 Tokens per Sec: 2424.153076\n",
            "Epoch Step: 1901 Loss: 3.484488 Tokens per Sec: 2485.452148\n",
            "Epoch Step: 1951 Loss: 3.081660 Tokens per Sec: 2449.542480\n",
            "Epoch Step: 2001 Loss: 3.562747 Tokens per Sec: 2444.331543\n",
            "Epoch Step: 2051 Loss: 3.521058 Tokens per Sec: 2436.533691\n",
            "Epoch Step: 2101 Loss: 2.329605 Tokens per Sec: 2411.983398\n",
            "Epoch Step: 2151 Loss: 3.435819 Tokens per Sec: 2461.362305\n",
            "Epoch Step: 2201 Loss: 3.275901 Tokens per Sec: 2445.137207\n",
            "Epoch Step: 2251 Loss: 3.645902 Tokens per Sec: 2459.455078\n",
            "Epoch Step: 2301 Loss: 3.774068 Tokens per Sec: 2402.678223\n",
            "Epoch Step: 2351 Loss: 4.090503 Tokens per Sec: 2442.910889\n",
            "Epoch Step: 2401 Loss: 3.167488 Tokens per Sec: 2448.225586\n",
            "Epoch Step: 2451 Loss: 3.670762 Tokens per Sec: 2419.655273\n",
            "Epoch Step: 2501 Loss: 3.952093 Tokens per Sec: 2433.349854\n",
            "Epoch Step: 2551 Loss: 4.261252 Tokens per Sec: 2442.439453\n",
            "Epoch Step: 2601 Loss: 3.659215 Tokens per Sec: 2403.300293\n",
            "Epoch Step: 2651 Loss: 3.134219 Tokens per Sec: 2441.622803\n",
            "Epoch Step: 2701 Loss: 4.015443 Tokens per Sec: 2428.917969\n",
            "Epoch Step: 2751 Loss: 3.318087 Tokens per Sec: 2425.368652\n",
            "Epoch Step: 2801 Loss: 3.442468 Tokens per Sec: 2446.706299\n",
            "Epoch Step: 2851 Loss: 3.172844 Tokens per Sec: 2467.659668\n",
            "Epoch Step: 2901 Loss: 3.303600 Tokens per Sec: 2447.409424\n",
            "Epoch Step: 2951 Loss: 3.480753 Tokens per Sec: 2413.878418\n",
            "Epoch Step: 3001 Loss: 3.212314 Tokens per Sec: 2455.737549\n",
            "Epoch Step: 3051 Loss: 3.525056 Tokens per Sec: 2433.051270\n",
            "Epoch Step: 3101 Loss: 3.611826 Tokens per Sec: 2454.668701\n",
            "Epoch Step: 3151 Loss: 3.754920 Tokens per Sec: 2441.344238\n",
            "Epoch Step: 3201 Loss: 2.982674 Tokens per Sec: 2429.576172\n",
            "Epoch Step: 3251 Loss: 3.138385 Tokens per Sec: 2427.176270\n",
            "Epoch Step: 3301 Loss: 2.716253 Tokens per Sec: 2405.692871\n",
            "Epoch Step: 3351 Loss: 2.988178 Tokens per Sec: 2461.019531\n",
            "Epoch Step: 3401 Loss: 2.948133 Tokens per Sec: 2463.756836\n",
            "Epoch Step: 3451 Loss: 3.917092 Tokens per Sec: 2476.507568\n",
            "Epoch Step: 3501 Loss: 3.363588 Tokens per Sec: 2447.036621\n",
            "Epoch Step: 3551 Loss: 4.068087 Tokens per Sec: 2467.253662\n",
            "Epoch Step: 3601 Loss: 2.816987 Tokens per Sec: 2474.074951\n",
            "Epoch Step: 3651 Loss: 3.996238 Tokens per Sec: 2471.508301\n",
            "Epoch Step: 3701 Loss: 2.870205 Tokens per Sec: 2449.384033\n",
            "Epoch Step: 3751 Loss: 3.562750 Tokens per Sec: 2450.165527\n",
            "Epoch Step: 3801 Loss: 3.771144 Tokens per Sec: 2452.480469\n",
            "Epoch Step: 3851 Loss: 3.454978 Tokens per Sec: 2486.235840\n",
            "Epoch Step: 3901 Loss: 2.661436 Tokens per Sec: 2436.923096\n",
            "Epoch Step: 3951 Loss: 4.043718 Tokens per Sec: 2423.618652\n",
            "Epoch Step: 4001 Loss: 3.451959 Tokens per Sec: 2481.264648\n",
            "Epoch Step: 4051 Loss: 3.481480 Tokens per Sec: 2492.900391\n",
            "Epoch Step: 4101 Loss: 3.533215 Tokens per Sec: 2391.369873\n",
            "Epoch Step: 4151 Loss: 3.624876 Tokens per Sec: 2422.793701\n",
            "Epoch Step: 4201 Loss: 3.085199 Tokens per Sec: 2428.610596\n",
            "Epoch Step: 4251 Loss: 3.422447 Tokens per Sec: 2453.008301\n",
            "Epoch Step: 4301 Loss: 3.991578 Tokens per Sec: 2444.098877\n",
            "Epoch Step: 4351 Loss: 3.388487 Tokens per Sec: 2422.454346\n",
            "Epoch Step: 4401 Loss: 2.856513 Tokens per Sec: 2428.180664\n",
            "Epoch Step: 4451 Loss: 3.416637 Tokens per Sec: 2459.190186\n",
            "Epoch Step: 4501 Loss: 3.260208 Tokens per Sec: 2429.073486\n",
            "Epoch Step: 1 Loss: 2.664486 Tokens per Sec: 2106.892334\n",
            "tensor(3.2672, device='cuda:0')\n",
            "Ending epoch 2 ...\n",
            "Starting epoch 3 ...\n",
            "Epoch Step: 1 Loss: 2.914424 Tokens per Sec: 1221.334473\n",
            "Epoch Step: 51 Loss: 3.026254 Tokens per Sec: 2462.142578\n",
            "Epoch Step: 101 Loss: 3.724552 Tokens per Sec: 2455.154053\n",
            "Epoch Step: 151 Loss: 3.208560 Tokens per Sec: 2453.883545\n",
            "Epoch Step: 201 Loss: 3.400970 Tokens per Sec: 2441.769775\n",
            "Epoch Step: 251 Loss: 4.266276 Tokens per Sec: 2407.438477\n",
            "Epoch Step: 301 Loss: 3.780750 Tokens per Sec: 2451.375977\n",
            "Epoch Step: 351 Loss: 3.276879 Tokens per Sec: 2395.806396\n",
            "Epoch Step: 401 Loss: 3.705833 Tokens per Sec: 2481.205078\n",
            "Epoch Step: 451 Loss: 3.572723 Tokens per Sec: 2413.468506\n",
            "Epoch Step: 501 Loss: 3.776585 Tokens per Sec: 2433.719727\n",
            "Epoch Step: 551 Loss: 3.827452 Tokens per Sec: 2464.665527\n",
            "Epoch Step: 601 Loss: 3.878565 Tokens per Sec: 2469.984131\n",
            "Epoch Step: 651 Loss: 3.670759 Tokens per Sec: 2432.616699\n",
            "Epoch Step: 701 Loss: 3.273068 Tokens per Sec: 2426.629639\n",
            "Epoch Step: 751 Loss: 3.964561 Tokens per Sec: 2419.655273\n",
            "Epoch Step: 801 Loss: 3.524081 Tokens per Sec: 2463.521240\n",
            "Epoch Step: 851 Loss: 3.863295 Tokens per Sec: 2464.809570\n",
            "Epoch Step: 901 Loss: 3.457988 Tokens per Sec: 2477.857910\n",
            "Epoch Step: 951 Loss: 3.960704 Tokens per Sec: 2433.149902\n",
            "Epoch Step: 1001 Loss: 3.367325 Tokens per Sec: 2486.167236\n",
            "Epoch Step: 1051 Loss: 4.216526 Tokens per Sec: 2382.006348\n",
            "Epoch Step: 1101 Loss: 3.698492 Tokens per Sec: 2476.361816\n",
            "Epoch Step: 1151 Loss: 3.234763 Tokens per Sec: 2445.771484\n",
            "Epoch Step: 1201 Loss: 4.078327 Tokens per Sec: 2435.516357\n",
            "Epoch Step: 1251 Loss: 3.444499 Tokens per Sec: 2450.994873\n",
            "Epoch Step: 1301 Loss: 3.617614 Tokens per Sec: 2450.954834\n",
            "Epoch Step: 1351 Loss: 2.819036 Tokens per Sec: 2431.727051\n",
            "Epoch Step: 1401 Loss: 3.427461 Tokens per Sec: 2424.464844\n",
            "Epoch Step: 1451 Loss: 4.135361 Tokens per Sec: 2452.970215\n",
            "Epoch Step: 1501 Loss: 3.425199 Tokens per Sec: 2493.553711\n",
            "Epoch Step: 1551 Loss: 2.707810 Tokens per Sec: 2379.291260\n",
            "Epoch Step: 1601 Loss: 3.997366 Tokens per Sec: 2465.563232\n",
            "Epoch Step: 1651 Loss: 3.281492 Tokens per Sec: 2458.675537\n",
            "Epoch Step: 1701 Loss: 3.544712 Tokens per Sec: 2452.140137\n",
            "Epoch Step: 1751 Loss: 3.948008 Tokens per Sec: 2421.769775\n",
            "Epoch Step: 1801 Loss: 3.383289 Tokens per Sec: 2469.001465\n",
            "Epoch Step: 1851 Loss: 2.802623 Tokens per Sec: 2403.714600\n",
            "Epoch Step: 1901 Loss: 4.092015 Tokens per Sec: 2463.642578\n",
            "Epoch Step: 1951 Loss: 3.470392 Tokens per Sec: 2417.855469\n",
            "Epoch Step: 2001 Loss: 3.514656 Tokens per Sec: 2477.968018\n",
            "Epoch Step: 2051 Loss: 3.588851 Tokens per Sec: 2429.296631\n",
            "Epoch Step: 2101 Loss: 3.635477 Tokens per Sec: 2464.292725\n",
            "Epoch Step: 2151 Loss: 3.674217 Tokens per Sec: 2461.288818\n",
            "Epoch Step: 2201 Loss: 3.697263 Tokens per Sec: 2477.313232\n",
            "Epoch Step: 2251 Loss: 3.262287 Tokens per Sec: 2422.859375\n",
            "Epoch Step: 2301 Loss: 3.978566 Tokens per Sec: 2408.519287\n",
            "Epoch Step: 2351 Loss: 2.985668 Tokens per Sec: 2408.443848\n",
            "Epoch Step: 2401 Loss: 3.162758 Tokens per Sec: 2453.843750\n",
            "Epoch Step: 2451 Loss: 3.020157 Tokens per Sec: 2462.786133\n",
            "Epoch Step: 2501 Loss: 3.508268 Tokens per Sec: 2417.516113\n",
            "Epoch Step: 2551 Loss: 3.282627 Tokens per Sec: 2446.584473\n",
            "Epoch Step: 2601 Loss: 3.095466 Tokens per Sec: 2460.768555\n",
            "Epoch Step: 2651 Loss: 3.347006 Tokens per Sec: 2440.274170\n",
            "Epoch Step: 2701 Loss: 3.224007 Tokens per Sec: 2406.669189\n",
            "Epoch Step: 2751 Loss: 3.563915 Tokens per Sec: 2435.251953\n",
            "Epoch Step: 2801 Loss: 3.269682 Tokens per Sec: 2452.727783\n",
            "Epoch Step: 2851 Loss: 3.369944 Tokens per Sec: 2426.655273\n",
            "Epoch Step: 2901 Loss: 3.773844 Tokens per Sec: 2447.461670\n",
            "Epoch Step: 2951 Loss: 3.588717 Tokens per Sec: 2423.862793\n",
            "Epoch Step: 3001 Loss: 3.476979 Tokens per Sec: 2426.389648\n",
            "Epoch Step: 3051 Loss: 3.284912 Tokens per Sec: 2432.375732\n",
            "Epoch Step: 3101 Loss: 3.471037 Tokens per Sec: 2429.376465\n",
            "Epoch Step: 3151 Loss: 3.487854 Tokens per Sec: 2373.277344\n",
            "Epoch Step: 3201 Loss: 3.101196 Tokens per Sec: 2456.131592\n",
            "Epoch Step: 3251 Loss: 3.336900 Tokens per Sec: 2447.080322\n",
            "Epoch Step: 3301 Loss: 2.839996 Tokens per Sec: 2476.452637\n",
            "Epoch Step: 3351 Loss: 3.250843 Tokens per Sec: 2458.939941\n",
            "Epoch Step: 3401 Loss: 2.839040 Tokens per Sec: 2436.791504\n",
            "Epoch Step: 3451 Loss: 2.768014 Tokens per Sec: 2350.056152\n",
            "Epoch Step: 3501 Loss: 2.338715 Tokens per Sec: 2442.407471\n",
            "Epoch Step: 3551 Loss: 3.850711 Tokens per Sec: 2457.885254\n",
            "Epoch Step: 3601 Loss: 2.853107 Tokens per Sec: 2474.831787\n",
            "Epoch Step: 3651 Loss: 2.840844 Tokens per Sec: 2455.336670\n",
            "Epoch Step: 3701 Loss: 3.641318 Tokens per Sec: 2378.544922\n",
            "Epoch Step: 3751 Loss: 2.675261 Tokens per Sec: 2468.701172\n",
            "Epoch Step: 3801 Loss: 3.377857 Tokens per Sec: 2464.822510\n",
            "Epoch Step: 3851 Loss: 3.329081 Tokens per Sec: 2418.515869\n",
            "Epoch Step: 3901 Loss: 3.790660 Tokens per Sec: 2421.029541\n",
            "Epoch Step: 3951 Loss: 3.878985 Tokens per Sec: 2465.200439\n",
            "Epoch Step: 4001 Loss: 3.666434 Tokens per Sec: 2477.715088\n",
            "Epoch Step: 4051 Loss: 3.078213 Tokens per Sec: 2444.357666\n",
            "Epoch Step: 4101 Loss: 3.456221 Tokens per Sec: 2465.593506\n",
            "Epoch Step: 4151 Loss: 3.146909 Tokens per Sec: 2419.929443\n",
            "Epoch Step: 4201 Loss: 3.646255 Tokens per Sec: 2404.756348\n",
            "Epoch Step: 4251 Loss: 3.874423 Tokens per Sec: 2419.507324\n",
            "Epoch Step: 4301 Loss: 3.835846 Tokens per Sec: 2435.549561\n",
            "Epoch Step: 4351 Loss: 3.166219 Tokens per Sec: 2460.644043\n",
            "Epoch Step: 4401 Loss: 3.212011 Tokens per Sec: 2474.371094\n",
            "Epoch Step: 4451 Loss: 2.553882 Tokens per Sec: 2430.277832\n",
            "Epoch Step: 4501 Loss: 3.501563 Tokens per Sec: 2442.355957\n",
            "Epoch Step: 1 Loss: 2.590747 Tokens per Sec: 2018.351929\n",
            "tensor(3.1339, device='cuda:0')\n",
            "Ending epoch 3 ...\n",
            "Starting epoch 4 ...\n",
            "Epoch Step: 1 Loss: 3.067922 Tokens per Sec: 1176.104126\n",
            "Epoch Step: 51 Loss: 3.728659 Tokens per Sec: 2429.893555\n",
            "Epoch Step: 101 Loss: 2.913985 Tokens per Sec: 2412.805664\n",
            "Epoch Step: 151 Loss: 3.076579 Tokens per Sec: 2477.975586\n",
            "Epoch Step: 201 Loss: 3.456202 Tokens per Sec: 2446.863037\n",
            "Epoch Step: 251 Loss: 2.772050 Tokens per Sec: 2442.290039\n",
            "Epoch Step: 301 Loss: 2.775424 Tokens per Sec: 2463.720459\n",
            "Epoch Step: 351 Loss: 3.152297 Tokens per Sec: 2491.884766\n",
            "Epoch Step: 401 Loss: 3.355637 Tokens per Sec: 2430.213379\n",
            "Epoch Step: 451 Loss: 3.007331 Tokens per Sec: 2469.972656\n",
            "Epoch Step: 501 Loss: 3.486335 Tokens per Sec: 2426.416504\n",
            "Epoch Step: 551 Loss: 3.014857 Tokens per Sec: 2430.939209\n",
            "Epoch Step: 601 Loss: 3.536944 Tokens per Sec: 2450.253174\n",
            "Epoch Step: 651 Loss: 3.225135 Tokens per Sec: 2433.240967\n",
            "Epoch Step: 701 Loss: 3.655632 Tokens per Sec: 2424.110352\n",
            "Epoch Step: 751 Loss: 3.467693 Tokens per Sec: 2434.580566\n",
            "Epoch Step: 801 Loss: 3.589437 Tokens per Sec: 2469.145752\n",
            "Epoch Step: 851 Loss: 3.259909 Tokens per Sec: 2456.462158\n",
            "Epoch Step: 901 Loss: 2.848997 Tokens per Sec: 2414.513184\n",
            "Epoch Step: 951 Loss: 3.635139 Tokens per Sec: 2451.440674\n",
            "Epoch Step: 1001 Loss: 3.520362 Tokens per Sec: 2435.399658\n",
            "Epoch Step: 1051 Loss: 3.022637 Tokens per Sec: 2440.182373\n",
            "Epoch Step: 1101 Loss: 3.891301 Tokens per Sec: 2392.185547\n",
            "Epoch Step: 1151 Loss: 2.784749 Tokens per Sec: 2441.231689\n",
            "Epoch Step: 1201 Loss: 2.508389 Tokens per Sec: 2477.162109\n",
            "Epoch Step: 1251 Loss: 2.942614 Tokens per Sec: 2421.108154\n",
            "Epoch Step: 1301 Loss: 3.853726 Tokens per Sec: 2394.211426\n",
            "Epoch Step: 1351 Loss: 3.741377 Tokens per Sec: 2449.550537\n",
            "Epoch Step: 1401 Loss: 2.571188 Tokens per Sec: 2432.821777\n",
            "Epoch Step: 1451 Loss: 2.634752 Tokens per Sec: 2410.731934\n",
            "Epoch Step: 1501 Loss: 3.642582 Tokens per Sec: 2426.757812\n",
            "Epoch Step: 1551 Loss: 4.070317 Tokens per Sec: 2447.887207\n",
            "Epoch Step: 1601 Loss: 3.192926 Tokens per Sec: 2478.420410\n",
            "Epoch Step: 1651 Loss: 3.486558 Tokens per Sec: 2392.922607\n",
            "Epoch Step: 1701 Loss: 3.166203 Tokens per Sec: 2428.096436\n",
            "Epoch Step: 1801 Loss: 3.843017 Tokens per Sec: 2470.492920\n",
            "Epoch Step: 1851 Loss: 2.911262 Tokens per Sec: 2481.184082\n",
            "Epoch Step: 1901 Loss: 3.368727 Tokens per Sec: 2447.941406\n",
            "Epoch Step: 1951 Loss: 3.392844 Tokens per Sec: 2444.039551\n",
            "Epoch Step: 2001 Loss: 3.186210 Tokens per Sec: 2471.539307\n",
            "Epoch Step: 2051 Loss: 2.792021 Tokens per Sec: 2432.893066\n",
            "Epoch Step: 2101 Loss: 3.270967 Tokens per Sec: 2449.097900\n",
            "Epoch Step: 2151 Loss: 3.044721 Tokens per Sec: 2456.630371\n",
            "Epoch Step: 2201 Loss: 2.841340 Tokens per Sec: 2469.902832\n",
            "Epoch Step: 2251 Loss: 3.470758 Tokens per Sec: 2445.506104\n",
            "Epoch Step: 2301 Loss: 3.452542 Tokens per Sec: 2391.690186\n",
            "Epoch Step: 2351 Loss: 2.485712 Tokens per Sec: 2471.932861\n",
            "Epoch Step: 2401 Loss: 3.108522 Tokens per Sec: 2446.598877\n",
            "Epoch Step: 2451 Loss: 3.577204 Tokens per Sec: 2477.801758\n",
            "Epoch Step: 2501 Loss: 3.779836 Tokens per Sec: 2462.753418\n",
            "Epoch Step: 2551 Loss: 3.085481 Tokens per Sec: 2422.315918\n",
            "Epoch Step: 2601 Loss: 3.515948 Tokens per Sec: 2432.892090\n",
            "Epoch Step: 2651 Loss: 2.902390 Tokens per Sec: 2461.150146\n",
            "Epoch Step: 2701 Loss: 2.837092 Tokens per Sec: 2452.946289\n",
            "Epoch Step: 2751 Loss: 3.110835 Tokens per Sec: 2412.619385\n",
            "Epoch Step: 2801 Loss: 3.378927 Tokens per Sec: 2464.234375\n",
            "Epoch Step: 2851 Loss: 3.612537 Tokens per Sec: 2417.624268\n",
            "Epoch Step: 2901 Loss: 3.781837 Tokens per Sec: 2385.220703\n",
            "Epoch Step: 2951 Loss: 2.809537 Tokens per Sec: 2440.272217\n",
            "Epoch Step: 3001 Loss: 3.219137 Tokens per Sec: 2442.541260\n",
            "Epoch Step: 3051 Loss: 3.168303 Tokens per Sec: 2462.656738\n",
            "Epoch Step: 3101 Loss: 2.992365 Tokens per Sec: 2439.114258\n",
            "Epoch Step: 3151 Loss: 2.698209 Tokens per Sec: 2401.942383\n",
            "Epoch Step: 3201 Loss: 3.357362 Tokens per Sec: 2409.947021\n",
            "Epoch Step: 3251 Loss: 2.941770 Tokens per Sec: 2438.863037\n",
            "Epoch Step: 3301 Loss: 2.988737 Tokens per Sec: 2411.001221\n",
            "Epoch Step: 3351 Loss: 2.456050 Tokens per Sec: 2450.415039\n",
            "Epoch Step: 3401 Loss: 4.005516 Tokens per Sec: 2449.370605\n",
            "Epoch Step: 3451 Loss: 2.975223 Tokens per Sec: 2430.923828\n",
            "Epoch Step: 3501 Loss: 2.635990 Tokens per Sec: 2440.058105\n",
            "Epoch Step: 3551 Loss: 3.620450 Tokens per Sec: 2407.313477\n",
            "Epoch Step: 3601 Loss: 2.638137 Tokens per Sec: 2435.483398\n",
            "Epoch Step: 3651 Loss: 3.847978 Tokens per Sec: 2481.994629\n",
            "Epoch Step: 3701 Loss: 0.848169 Tokens per Sec: 2413.001221\n",
            "Epoch Step: 3751 Loss: 3.351591 Tokens per Sec: 2443.325195\n",
            "Epoch Step: 3801 Loss: 3.319633 Tokens per Sec: 2450.941406\n",
            "Epoch Step: 3851 Loss: 3.007280 Tokens per Sec: 2469.049072\n",
            "Epoch Step: 3901 Loss: 3.106314 Tokens per Sec: 2434.227051\n",
            "Epoch Step: 3951 Loss: 3.146512 Tokens per Sec: 2422.366699\n",
            "Epoch Step: 4001 Loss: 3.026247 Tokens per Sec: 2460.078369\n",
            "Epoch Step: 4051 Loss: 3.455494 Tokens per Sec: 2402.013916\n",
            "Epoch Step: 4101 Loss: 3.363784 Tokens per Sec: 2429.892090\n",
            "Epoch Step: 4151 Loss: 3.733777 Tokens per Sec: 2498.106689\n",
            "Epoch Step: 4201 Loss: 3.142977 Tokens per Sec: 2450.793457\n",
            "Epoch Step: 4251 Loss: 3.344294 Tokens per Sec: 2447.478760\n",
            "Epoch Step: 4301 Loss: 3.452546 Tokens per Sec: 2419.700195\n",
            "Epoch Step: 4351 Loss: 3.261748 Tokens per Sec: 2426.879395\n",
            "Epoch Step: 4401 Loss: 3.356013 Tokens per Sec: 2435.896240\n",
            "Epoch Step: 4451 Loss: 3.076840 Tokens per Sec: 2465.760254\n",
            "Epoch Step: 4501 Loss: 3.388385 Tokens per Sec: 2438.975830\n",
            "Epoch Step: 1 Loss: 2.492584 Tokens per Sec: 2008.590454\n",
            "tensor(3.0384, device='cuda:0')\n",
            "Ending epoch 4 ...\n",
            "Starting epoch 5 ...\n",
            "Epoch Step: 1 Loss: 2.978735 Tokens per Sec: 1131.407227\n",
            "Epoch Step: 51 Loss: 3.221976 Tokens per Sec: 2441.589111\n",
            "Epoch Step: 101 Loss: 3.288382 Tokens per Sec: 2398.207520\n",
            "Epoch Step: 151 Loss: 2.994841 Tokens per Sec: 2455.479004\n",
            "Epoch Step: 201 Loss: 3.143641 Tokens per Sec: 2453.980225\n",
            "Epoch Step: 251 Loss: 2.536533 Tokens per Sec: 2453.566162\n",
            "Epoch Step: 301 Loss: 2.732512 Tokens per Sec: 2379.882080\n",
            "Epoch Step: 351 Loss: 2.841424 Tokens per Sec: 2415.547852\n",
            "Epoch Step: 401 Loss: 3.342942 Tokens per Sec: 2436.178223\n",
            "Epoch Step: 451 Loss: 3.452945 Tokens per Sec: 2397.454834\n",
            "Epoch Step: 501 Loss: 3.197097 Tokens per Sec: 2405.704834\n",
            "Epoch Step: 551 Loss: 3.028867 Tokens per Sec: 2437.386719\n",
            "Epoch Step: 601 Loss: 3.454670 Tokens per Sec: 2456.972900\n",
            "Epoch Step: 651 Loss: 3.576690 Tokens per Sec: 2451.124268\n",
            "Epoch Step: 701 Loss: 3.551285 Tokens per Sec: 2458.500977\n",
            "Epoch Step: 751 Loss: 3.130552 Tokens per Sec: 2455.327881\n",
            "Epoch Step: 801 Loss: 3.146444 Tokens per Sec: 2428.921875\n",
            "Epoch Step: 851 Loss: 3.221516 Tokens per Sec: 2411.706299\n",
            "Epoch Step: 901 Loss: 3.513260 Tokens per Sec: 2443.480225\n",
            "Epoch Step: 951 Loss: 3.224854 Tokens per Sec: 2446.169922\n",
            "Epoch Step: 1001 Loss: 3.322364 Tokens per Sec: 2467.557373\n",
            "Epoch Step: 1051 Loss: 2.868340 Tokens per Sec: 2454.754883\n",
            "Epoch Step: 1101 Loss: 3.341770 Tokens per Sec: 2459.591064\n",
            "Epoch Step: 1151 Loss: 3.178630 Tokens per Sec: 2397.111572\n",
            "Epoch Step: 1201 Loss: 3.813193 Tokens per Sec: 2477.160400\n",
            "Epoch Step: 1251 Loss: 2.987331 Tokens per Sec: 2437.899414\n",
            "Epoch Step: 1301 Loss: 3.403490 Tokens per Sec: 2408.402832\n",
            "Epoch Step: 1351 Loss: 3.009626 Tokens per Sec: 2452.571289\n",
            "Epoch Step: 1401 Loss: 3.155044 Tokens per Sec: 2413.575195\n",
            "Epoch Step: 1451 Loss: 3.187372 Tokens per Sec: 2445.778564\n",
            "Epoch Step: 1501 Loss: 3.255870 Tokens per Sec: 2469.207275\n",
            "Epoch Step: 1551 Loss: 2.923295 Tokens per Sec: 2440.102539\n",
            "Epoch Step: 1601 Loss: 2.647738 Tokens per Sec: 2456.169434\n",
            "Epoch Step: 1651 Loss: 3.238880 Tokens per Sec: 2478.998291\n",
            "Epoch Step: 1701 Loss: 3.283937 Tokens per Sec: 2410.973145\n",
            "Epoch Step: 1751 Loss: 3.538605 Tokens per Sec: 2441.938721\n",
            "Epoch Step: 1801 Loss: 3.254359 Tokens per Sec: 2446.048340\n",
            "Epoch Step: 1851 Loss: 2.590996 Tokens per Sec: 2426.945068\n",
            "Epoch Step: 1901 Loss: 3.493865 Tokens per Sec: 2466.019043\n",
            "Epoch Step: 1951 Loss: 2.604490 Tokens per Sec: 2457.595947\n",
            "Epoch Step: 2001 Loss: 3.065638 Tokens per Sec: 2435.711426\n",
            "Epoch Step: 2051 Loss: 3.634817 Tokens per Sec: 2432.424072\n",
            "Epoch Step: 2101 Loss: 2.571611 Tokens per Sec: 2446.918701\n",
            "Epoch Step: 2151 Loss: 2.705943 Tokens per Sec: 2432.779785\n",
            "Epoch Step: 2201 Loss: 3.349172 Tokens per Sec: 2452.963379\n",
            "Epoch Step: 2251 Loss: 2.920918 Tokens per Sec: 2453.073730\n",
            "Epoch Step: 2301 Loss: 3.309378 Tokens per Sec: 2410.304443\n",
            "Epoch Step: 2351 Loss: 3.347719 Tokens per Sec: 2451.504395\n",
            "Epoch Step: 2401 Loss: 2.751881 Tokens per Sec: 2438.996826\n",
            "Epoch Step: 2451 Loss: 1.900998 Tokens per Sec: 2402.224365\n",
            "Epoch Step: 2501 Loss: 3.407187 Tokens per Sec: 2431.204834\n",
            "Epoch Step: 2551 Loss: 2.532367 Tokens per Sec: 2427.665771\n",
            "Epoch Step: 2601 Loss: 2.574301 Tokens per Sec: 2459.270020\n",
            "Epoch Step: 2651 Loss: 3.450174 Tokens per Sec: 2451.016602\n",
            "Epoch Step: 2701 Loss: 4.174884 Tokens per Sec: 2446.881836\n",
            "Epoch Step: 2751 Loss: 3.020980 Tokens per Sec: 2413.716064\n",
            "Epoch Step: 2801 Loss: 2.502479 Tokens per Sec: 2450.167969\n",
            "Epoch Step: 2851 Loss: 3.535813 Tokens per Sec: 2438.996094\n",
            "Epoch Step: 2901 Loss: 3.627019 Tokens per Sec: 2414.510742\n",
            "Epoch Step: 2951 Loss: 3.063784 Tokens per Sec: 2467.265381\n",
            "Epoch Step: 3001 Loss: 3.170526 Tokens per Sec: 2451.654297\n",
            "Epoch Step: 3051 Loss: 3.199571 Tokens per Sec: 2437.937744\n",
            "Epoch Step: 3101 Loss: 3.793602 Tokens per Sec: 2418.829590\n",
            "Epoch Step: 3151 Loss: 3.220906 Tokens per Sec: 2441.166504\n",
            "Epoch Step: 3201 Loss: 2.878881 Tokens per Sec: 2492.684570\n",
            "Epoch Step: 3251 Loss: 2.871505 Tokens per Sec: 2436.061523\n",
            "Epoch Step: 3301 Loss: 3.395371 Tokens per Sec: 2375.914795\n",
            "Epoch Step: 3351 Loss: 2.123008 Tokens per Sec: 2435.498535\n",
            "Epoch Step: 3401 Loss: 2.931572 Tokens per Sec: 2463.513428\n",
            "Epoch Step: 3451 Loss: 3.187060 Tokens per Sec: 2367.968506\n",
            "Epoch Step: 3501 Loss: 2.681946 Tokens per Sec: 2413.527100\n",
            "Epoch Step: 3551 Loss: 4.025028 Tokens per Sec: 2431.751709\n",
            "Epoch Step: 3601 Loss: 2.566064 Tokens per Sec: 2378.998779\n",
            "Epoch Step: 3651 Loss: 3.782618 Tokens per Sec: 2396.175293\n",
            "Epoch Step: 3701 Loss: 3.302026 Tokens per Sec: 2393.846436\n",
            "Epoch Step: 3751 Loss: 3.251893 Tokens per Sec: 2413.876709\n",
            "Epoch Step: 3801 Loss: 3.779418 Tokens per Sec: 2455.629883\n",
            "Epoch Step: 3851 Loss: 3.218698 Tokens per Sec: 2460.034912\n",
            "Epoch Step: 3901 Loss: 3.244550 Tokens per Sec: 2430.652100\n",
            "Epoch Step: 3951 Loss: 3.243672 Tokens per Sec: 2416.023926\n",
            "Epoch Step: 4001 Loss: 3.204957 Tokens per Sec: 2448.777832\n",
            "Epoch Step: 4051 Loss: 2.413776 Tokens per Sec: 2445.134766\n",
            "Epoch Step: 4101 Loss: 3.440076 Tokens per Sec: 2445.867188\n",
            "Epoch Step: 4151 Loss: 2.889574 Tokens per Sec: 2454.890381\n",
            "Epoch Step: 4201 Loss: 3.742340 Tokens per Sec: 2442.127686\n",
            "Epoch Step: 4251 Loss: 3.637192 Tokens per Sec: 2452.605713\n",
            "Epoch Step: 4301 Loss: 2.569406 Tokens per Sec: 2479.270508\n",
            "Epoch Step: 4351 Loss: 3.457397 Tokens per Sec: 2480.369141\n",
            "Epoch Step: 4401 Loss: 3.381420 Tokens per Sec: 2437.871338\n",
            "Epoch Step: 4451 Loss: 3.050199 Tokens per Sec: 2410.401611\n",
            "Epoch Step: 4501 Loss: 3.738941 Tokens per Sec: 2465.542480\n",
            "Epoch Step: 1 Loss: 2.374318 Tokens per Sec: 2105.781006\n",
            "tensor(2.9506, device='cuda:0')\n",
            "Ending epoch 5 ...\n",
            "Starting epoch 6 ...\n",
            "Epoch Step: 1 Loss: 2.787258 Tokens per Sec: 1204.261108\n",
            "Epoch Step: 51 Loss: 2.186502 Tokens per Sec: 2429.722412\n",
            "Epoch Step: 101 Loss: 3.248887 Tokens per Sec: 2422.902344\n",
            "Epoch Step: 151 Loss: 2.052309 Tokens per Sec: 2426.884277\n",
            "Epoch Step: 201 Loss: 3.179243 Tokens per Sec: 2384.793701\n",
            "Epoch Step: 251 Loss: 3.179135 Tokens per Sec: 2419.856934\n",
            "Epoch Step: 301 Loss: 2.924817 Tokens per Sec: 2451.460938\n",
            "Epoch Step: 351 Loss: 3.508454 Tokens per Sec: 2410.053955\n",
            "Epoch Step: 401 Loss: 3.323821 Tokens per Sec: 2442.417236\n",
            "Epoch Step: 451 Loss: 2.941705 Tokens per Sec: 2417.848389\n",
            "Epoch Step: 501 Loss: 3.621494 Tokens per Sec: 2454.949463\n",
            "Epoch Step: 551 Loss: 3.894480 Tokens per Sec: 2441.106445\n",
            "Epoch Step: 601 Loss: 3.609270 Tokens per Sec: 2429.059326\n",
            "Epoch Step: 651 Loss: 3.128121 Tokens per Sec: 2454.694824\n",
            "Epoch Step: 701 Loss: 2.944087 Tokens per Sec: 2433.865234\n",
            "Epoch Step: 751 Loss: 2.588261 Tokens per Sec: 2417.045166\n",
            "Epoch Step: 801 Loss: 3.061050 Tokens per Sec: 2429.037109\n",
            "Epoch Step: 851 Loss: 3.166181 Tokens per Sec: 2405.965088\n",
            "Epoch Step: 901 Loss: 2.939646 Tokens per Sec: 2415.334473\n",
            "Epoch Step: 951 Loss: 3.109432 Tokens per Sec: 2452.852539\n",
            "Epoch Step: 1001 Loss: 3.557114 Tokens per Sec: 2460.258057\n",
            "Epoch Step: 1051 Loss: 3.407917 Tokens per Sec: 2435.302490\n",
            "Epoch Step: 1101 Loss: 3.108480 Tokens per Sec: 2439.692139\n",
            "Epoch Step: 1151 Loss: 3.122841 Tokens per Sec: 2467.527344\n",
            "Epoch Step: 1201 Loss: 2.717482 Tokens per Sec: 2374.878906\n",
            "Epoch Step: 1251 Loss: 2.811482 Tokens per Sec: 2390.664062\n",
            "Epoch Step: 1301 Loss: 3.632832 Tokens per Sec: 2399.740967\n",
            "Epoch Step: 1351 Loss: 3.248258 Tokens per Sec: 2451.044678\n",
            "Epoch Step: 1401 Loss: 3.001446 Tokens per Sec: 2385.119385\n",
            "Epoch Step: 1451 Loss: 2.914326 Tokens per Sec: 2439.809326\n",
            "Epoch Step: 1501 Loss: 3.346539 Tokens per Sec: 2431.370117\n",
            "Epoch Step: 1551 Loss: 2.514746 Tokens per Sec: 2427.609619\n",
            "Epoch Step: 1601 Loss: 3.244703 Tokens per Sec: 2437.109863\n",
            "Epoch Step: 1651 Loss: 2.918040 Tokens per Sec: 2400.020996\n",
            "Epoch Step: 1701 Loss: 3.397756 Tokens per Sec: 2385.273193\n",
            "Epoch Step: 1751 Loss: 3.215958 Tokens per Sec: 2455.961670\n",
            "Epoch Step: 1801 Loss: 3.443141 Tokens per Sec: 2419.802246\n",
            "Epoch Step: 1851 Loss: 3.323909 Tokens per Sec: 2439.647461\n",
            "Epoch Step: 1901 Loss: 3.316467 Tokens per Sec: 2413.187256\n",
            "Epoch Step: 1951 Loss: 2.740122 Tokens per Sec: 2411.626953\n",
            "Epoch Step: 2001 Loss: 3.058470 Tokens per Sec: 2444.027832\n",
            "Epoch Step: 2051 Loss: 2.500144 Tokens per Sec: 2449.017090\n",
            "Epoch Step: 2101 Loss: 2.878446 Tokens per Sec: 2482.934082\n",
            "Epoch Step: 2151 Loss: 2.615295 Tokens per Sec: 2443.573975\n",
            "Epoch Step: 2201 Loss: 3.345816 Tokens per Sec: 2454.036133\n",
            "Epoch Step: 2251 Loss: 3.165114 Tokens per Sec: 2458.814697\n",
            "Epoch Step: 2301 Loss: 3.104947 Tokens per Sec: 2405.143799\n",
            "Epoch Step: 2351 Loss: 2.788828 Tokens per Sec: 2437.102295\n",
            "Epoch Step: 2401 Loss: 2.362089 Tokens per Sec: 2445.394531\n",
            "Epoch Step: 2451 Loss: 3.196166 Tokens per Sec: 2434.888672\n",
            "Epoch Step: 2501 Loss: 3.609953 Tokens per Sec: 2463.807129\n",
            "Epoch Step: 2551 Loss: 3.507682 Tokens per Sec: 2411.350586\n",
            "Epoch Step: 2601 Loss: 3.322857 Tokens per Sec: 2450.654297\n",
            "Epoch Step: 2651 Loss: 3.750620 Tokens per Sec: 2380.063232\n",
            "Epoch Step: 2701 Loss: 2.840391 Tokens per Sec: 2448.881348\n",
            "Epoch Step: 2751 Loss: 3.356077 Tokens per Sec: 2474.687256\n",
            "Epoch Step: 2801 Loss: 3.246722 Tokens per Sec: 2456.827393\n",
            "Epoch Step: 2851 Loss: 2.894645 Tokens per Sec: 2460.375488\n",
            "Epoch Step: 2901 Loss: 2.382689 Tokens per Sec: 2443.681396\n",
            "Epoch Step: 2951 Loss: 3.084774 Tokens per Sec: 2432.276611\n",
            "Epoch Step: 3001 Loss: 3.608012 Tokens per Sec: 2460.530273\n",
            "Epoch Step: 3051 Loss: 3.264589 Tokens per Sec: 2447.279297\n",
            "Epoch Step: 3101 Loss: 3.361678 Tokens per Sec: 2397.994629\n",
            "Epoch Step: 3151 Loss: 3.445987 Tokens per Sec: 2402.823242\n",
            "Epoch Step: 3201 Loss: 3.338531 Tokens per Sec: 2414.550781\n",
            "Epoch Step: 3251 Loss: 3.133283 Tokens per Sec: 2389.392578\n",
            "Epoch Step: 3301 Loss: 3.378666 Tokens per Sec: 2409.166992\n",
            "Epoch Step: 3351 Loss: 2.854511 Tokens per Sec: 2390.839111\n",
            "Epoch Step: 3401 Loss: 2.511332 Tokens per Sec: 2467.064697\n",
            "Epoch Step: 3451 Loss: 3.563394 Tokens per Sec: 2458.520996\n",
            "Epoch Step: 3501 Loss: 3.143175 Tokens per Sec: 2446.025635\n",
            "Epoch Step: 3551 Loss: 1.954438 Tokens per Sec: 2430.323486\n",
            "Epoch Step: 3601 Loss: 3.315506 Tokens per Sec: 2458.873535\n",
            "Epoch Step: 3651 Loss: 3.583789 Tokens per Sec: 2418.219971\n",
            "Epoch Step: 3701 Loss: 2.817091 Tokens per Sec: 2444.828613\n",
            "Epoch Step: 3751 Loss: 3.332596 Tokens per Sec: 2429.721924\n",
            "Epoch Step: 3801 Loss: 3.052244 Tokens per Sec: 2399.718262\n",
            "Epoch Step: 3851 Loss: 3.261398 Tokens per Sec: 2397.705078\n",
            "Epoch Step: 3901 Loss: 3.381967 Tokens per Sec: 2433.848145\n",
            "Epoch Step: 3951 Loss: 3.522404 Tokens per Sec: 2448.185303\n",
            "Epoch Step: 4001 Loss: 3.311193 Tokens per Sec: 2427.899658\n",
            "Epoch Step: 4051 Loss: 3.312706 Tokens per Sec: 2445.345703\n",
            "Epoch Step: 4101 Loss: 2.881981 Tokens per Sec: 2449.658936\n",
            "Epoch Step: 4151 Loss: 3.288377 Tokens per Sec: 2449.639160\n",
            "Epoch Step: 4201 Loss: 3.335520 Tokens per Sec: 2380.996582\n",
            "Epoch Step: 4251 Loss: 3.176883 Tokens per Sec: 2439.104736\n",
            "Epoch Step: 4301 Loss: 2.259957 Tokens per Sec: 2389.030518\n",
            "Epoch Step: 4351 Loss: 3.266414 Tokens per Sec: 2442.690186\n",
            "Epoch Step: 4401 Loss: 2.850727 Tokens per Sec: 2426.816406\n",
            "Epoch Step: 4451 Loss: 2.837157 Tokens per Sec: 2452.149414\n",
            "Epoch Step: 4501 Loss: 3.326528 Tokens per Sec: 2418.009766\n",
            "Epoch Step: 1 Loss: 2.287262 Tokens per Sec: 2033.766235\n",
            "tensor(2.8618, device='cuda:0')\n",
            "Ending epoch 6 ...\n",
            "Starting epoch 7 ...\n",
            "Epoch Step: 1 Loss: 2.934723 Tokens per Sec: 1168.179565\n",
            "Epoch Step: 51 Loss: 3.360286 Tokens per Sec: 2476.559082\n",
            "Epoch Step: 101 Loss: 2.609406 Tokens per Sec: 2443.912842\n",
            "Epoch Step: 151 Loss: 2.919269 Tokens per Sec: 2472.152588\n",
            "Epoch Step: 201 Loss: 3.750921 Tokens per Sec: 2405.453613\n",
            "Epoch Step: 251 Loss: 3.200878 Tokens per Sec: 2424.554688\n",
            "Epoch Step: 301 Loss: 2.535571 Tokens per Sec: 2463.786865\n",
            "Epoch Step: 351 Loss: 2.105292 Tokens per Sec: 2423.720947\n",
            "Epoch Step: 401 Loss: 2.338876 Tokens per Sec: 2398.884766\n",
            "Epoch Step: 451 Loss: 3.016546 Tokens per Sec: 2359.251221\n",
            "Epoch Step: 501 Loss: 2.643213 Tokens per Sec: 2461.539307\n",
            "Epoch Step: 551 Loss: 3.246970 Tokens per Sec: 2440.591309\n",
            "Epoch Step: 601 Loss: 3.181054 Tokens per Sec: 2435.759766\n",
            "Epoch Step: 651 Loss: 3.702996 Tokens per Sec: 2383.495361\n",
            "Epoch Step: 701 Loss: 2.995990 Tokens per Sec: 2447.508301\n",
            "Epoch Step: 751 Loss: 4.065372 Tokens per Sec: 2410.243896\n",
            "Epoch Step: 801 Loss: 3.353159 Tokens per Sec: 2423.246826\n",
            "Epoch Step: 851 Loss: 2.271449 Tokens per Sec: 2444.565918\n",
            "Epoch Step: 901 Loss: 3.437063 Tokens per Sec: 2441.341797\n",
            "Epoch Step: 951 Loss: 3.265754 Tokens per Sec: 2446.968994\n",
            "Epoch Step: 1001 Loss: 2.925813 Tokens per Sec: 2433.727051\n",
            "Epoch Step: 1051 Loss: 3.305969 Tokens per Sec: 2415.200439\n",
            "Epoch Step: 1101 Loss: 3.226686 Tokens per Sec: 2402.761230\n",
            "Epoch Step: 1151 Loss: 3.055608 Tokens per Sec: 2460.390625\n",
            "Epoch Step: 1201 Loss: 2.202544 Tokens per Sec: 2381.457275\n",
            "Epoch Step: 1251 Loss: 3.220749 Tokens per Sec: 2449.946045\n",
            "Epoch Step: 1301 Loss: 2.996462 Tokens per Sec: 2416.191895\n",
            "Epoch Step: 1351 Loss: 3.672291 Tokens per Sec: 2412.792969\n",
            "Epoch Step: 1401 Loss: 3.287529 Tokens per Sec: 2441.269775\n",
            "Epoch Step: 1451 Loss: 3.134138 Tokens per Sec: 2468.484619\n",
            "Epoch Step: 1501 Loss: 2.569601 Tokens per Sec: 2412.149170\n",
            "Epoch Step: 1551 Loss: 3.260331 Tokens per Sec: 2391.799072\n",
            "Epoch Step: 1601 Loss: 3.674718 Tokens per Sec: 2452.067139\n",
            "Epoch Step: 1651 Loss: 3.298192 Tokens per Sec: 2402.892334\n",
            "Epoch Step: 1701 Loss: 3.170328 Tokens per Sec: 2438.500732\n",
            "Epoch Step: 1751 Loss: 3.277406 Tokens per Sec: 2467.645020\n",
            "Epoch Step: 1801 Loss: 3.651804 Tokens per Sec: 2462.369385\n",
            "Epoch Step: 1851 Loss: 3.659593 Tokens per Sec: 2441.616211\n",
            "Epoch Step: 1901 Loss: 2.365973 Tokens per Sec: 2392.845703\n",
            "Epoch Step: 1951 Loss: 2.688415 Tokens per Sec: 2440.121094\n",
            "Epoch Step: 2001 Loss: 2.615571 Tokens per Sec: 2452.121826\n",
            "Epoch Step: 2051 Loss: 3.682969 Tokens per Sec: 2463.258057\n",
            "Epoch Step: 2101 Loss: 2.546129 Tokens per Sec: 2416.565430\n",
            "Epoch Step: 2151 Loss: 3.049177 Tokens per Sec: 2441.546143\n",
            "Epoch Step: 2201 Loss: 3.024366 Tokens per Sec: 2446.710693\n",
            "Epoch Step: 2251 Loss: 2.219958 Tokens per Sec: 2423.935303\n",
            "Epoch Step: 2301 Loss: 2.840264 Tokens per Sec: 2382.075439\n",
            "Epoch Step: 2351 Loss: 3.259186 Tokens per Sec: 2420.820312\n",
            "Epoch Step: 2401 Loss: 2.919257 Tokens per Sec: 2433.315186\n",
            "Epoch Step: 2451 Loss: 3.365603 Tokens per Sec: 2415.884521\n",
            "Epoch Step: 2501 Loss: 2.820315 Tokens per Sec: 2470.812256\n",
            "Epoch Step: 2551 Loss: 2.643638 Tokens per Sec: 2424.870361\n",
            "Epoch Step: 2601 Loss: 3.140003 Tokens per Sec: 2418.933838\n",
            "Epoch Step: 2651 Loss: 3.113977 Tokens per Sec: 2454.982910\n",
            "Epoch Step: 2701 Loss: 3.054944 Tokens per Sec: 2424.073975\n",
            "Epoch Step: 2751 Loss: 2.712687 Tokens per Sec: 2475.260254\n",
            "Epoch Step: 2801 Loss: 3.376254 Tokens per Sec: 2443.170654\n",
            "Epoch Step: 2851 Loss: 3.401911 Tokens per Sec: 2423.465088\n",
            "Epoch Step: 2901 Loss: 3.157084 Tokens per Sec: 2392.409668\n",
            "Epoch Step: 2951 Loss: 3.303049 Tokens per Sec: 2447.467041\n",
            "Epoch Step: 3001 Loss: 2.749351 Tokens per Sec: 2364.381592\n",
            "Epoch Step: 3051 Loss: 2.895119 Tokens per Sec: 2419.049561\n",
            "Epoch Step: 3101 Loss: 2.875761 Tokens per Sec: 2420.256104\n",
            "Epoch Step: 3151 Loss: 2.917132 Tokens per Sec: 2446.452393\n",
            "Epoch Step: 3201 Loss: 2.187533 Tokens per Sec: 2433.927002\n",
            "Epoch Step: 3251 Loss: 3.138551 Tokens per Sec: 2394.202881\n",
            "Epoch Step: 3301 Loss: 3.310436 Tokens per Sec: 2469.983398\n",
            "Epoch Step: 3351 Loss: 2.669682 Tokens per Sec: 2454.093750\n",
            "Epoch Step: 3401 Loss: 2.864533 Tokens per Sec: 2405.570068\n",
            "Epoch Step: 3451 Loss: 2.937549 Tokens per Sec: 2387.083252\n",
            "Epoch Step: 3501 Loss: 3.368867 Tokens per Sec: 2438.181152\n",
            "Epoch Step: 3551 Loss: 2.897858 Tokens per Sec: 2411.906494\n",
            "Epoch Step: 3601 Loss: 3.310725 Tokens per Sec: 2454.119385\n",
            "Epoch Step: 3651 Loss: 3.370210 Tokens per Sec: 2453.845703\n",
            "Epoch Step: 3701 Loss: 2.999102 Tokens per Sec: 2434.597168\n",
            "Epoch Step: 3751 Loss: 3.672876 Tokens per Sec: 2442.301758\n",
            "Epoch Step: 3801 Loss: 3.452272 Tokens per Sec: 2434.967285\n",
            "Epoch Step: 3851 Loss: 3.380821 Tokens per Sec: 2458.608887\n",
            "Epoch Step: 3901 Loss: 3.099809 Tokens per Sec: 2475.788574\n",
            "Epoch Step: 3951 Loss: 3.213257 Tokens per Sec: 2413.492432\n",
            "Epoch Step: 4001 Loss: 3.444370 Tokens per Sec: 2410.913574\n",
            "Epoch Step: 4051 Loss: 2.849284 Tokens per Sec: 2433.472168\n",
            "Epoch Step: 4101 Loss: 2.978263 Tokens per Sec: 2430.779297\n",
            "Epoch Step: 4151 Loss: 2.929372 Tokens per Sec: 2483.688965\n",
            "Epoch Step: 4201 Loss: 3.434520 Tokens per Sec: 2429.471191\n",
            "Epoch Step: 4251 Loss: 3.005525 Tokens per Sec: 2457.827148\n",
            "Epoch Step: 4301 Loss: 2.485228 Tokens per Sec: 2436.419434\n",
            "Epoch Step: 4351 Loss: 3.525208 Tokens per Sec: 2428.807373\n",
            "Epoch Step: 4401 Loss: 3.423825 Tokens per Sec: 2439.953125\n",
            "Epoch Step: 4451 Loss: 3.593493 Tokens per Sec: 2419.691162\n",
            "Epoch Step: 4501 Loss: 3.453736 Tokens per Sec: 2424.634521\n",
            "Epoch Step: 1 Loss: 2.218751 Tokens per Sec: 2078.543457\n",
            "tensor(2.7800, device='cuda:0')\n",
            "Ending epoch 7 ...\n",
            "Starting epoch 8 ...\n",
            "Epoch Step: 1 Loss: 3.113661 Tokens per Sec: 1201.105347\n",
            "Epoch Step: 51 Loss: 2.647652 Tokens per Sec: 2424.529541\n",
            "Epoch Step: 101 Loss: 3.077990 Tokens per Sec: 2407.267822\n",
            "Epoch Step: 151 Loss: 2.942658 Tokens per Sec: 2392.207764\n",
            "Epoch Step: 201 Loss: 3.277105 Tokens per Sec: 2451.170898\n",
            "Epoch Step: 251 Loss: 2.632637 Tokens per Sec: 2432.383545\n",
            "Epoch Step: 301 Loss: 2.524704 Tokens per Sec: 2428.475586\n",
            "Epoch Step: 351 Loss: 3.396038 Tokens per Sec: 2429.621582\n",
            "Epoch Step: 401 Loss: 2.598742 Tokens per Sec: 2357.259766\n",
            "Epoch Step: 451 Loss: 2.882138 Tokens per Sec: 2388.939697\n",
            "Epoch Step: 501 Loss: 2.770027 Tokens per Sec: 2465.345215\n",
            "Epoch Step: 551 Loss: 2.978915 Tokens per Sec: 2456.616455\n",
            "Epoch Step: 601 Loss: 2.341267 Tokens per Sec: 2475.135986\n",
            "Epoch Step: 651 Loss: 2.427962 Tokens per Sec: 2427.291992\n",
            "Epoch Step: 701 Loss: 2.957028 Tokens per Sec: 2461.704346\n",
            "Epoch Step: 751 Loss: 2.680421 Tokens per Sec: 2445.708496\n",
            "Epoch Step: 801 Loss: 2.456706 Tokens per Sec: 2423.309326\n",
            "Epoch Step: 851 Loss: 3.705677 Tokens per Sec: 2408.329590\n",
            "Epoch Step: 901 Loss: 2.488705 Tokens per Sec: 2421.239990\n",
            "Epoch Step: 951 Loss: 2.705154 Tokens per Sec: 2459.379639\n",
            "Epoch Step: 1001 Loss: 2.748788 Tokens per Sec: 2419.426025\n",
            "Epoch Step: 1051 Loss: 3.256831 Tokens per Sec: 2438.539551\n",
            "Epoch Step: 1101 Loss: 2.532905 Tokens per Sec: 2462.510498\n",
            "Epoch Step: 1151 Loss: 3.212462 Tokens per Sec: 2435.486572\n",
            "Epoch Step: 1201 Loss: 3.020817 Tokens per Sec: 2437.768311\n",
            "Epoch Step: 1251 Loss: 3.160572 Tokens per Sec: 2465.003174\n",
            "Epoch Step: 1301 Loss: 3.193747 Tokens per Sec: 2428.242676\n",
            "Epoch Step: 1351 Loss: 3.522607 Tokens per Sec: 2360.404297\n",
            "Epoch Step: 1401 Loss: 3.089547 Tokens per Sec: 2445.881104\n",
            "Epoch Step: 1451 Loss: 3.223101 Tokens per Sec: 2463.459717\n",
            "Epoch Step: 1501 Loss: 3.690033 Tokens per Sec: 2425.626709\n",
            "Epoch Step: 1551 Loss: 2.872629 Tokens per Sec: 2436.145264\n",
            "Epoch Step: 1601 Loss: 3.181856 Tokens per Sec: 2466.837891\n",
            "Epoch Step: 1651 Loss: 3.395071 Tokens per Sec: 2443.913818\n",
            "Epoch Step: 1701 Loss: 2.850621 Tokens per Sec: 2394.287598\n",
            "Epoch Step: 1751 Loss: 2.704787 Tokens per Sec: 2439.156494\n",
            "Epoch Step: 1801 Loss: 2.572482 Tokens per Sec: 2463.078369\n",
            "Epoch Step: 1851 Loss: 3.053047 Tokens per Sec: 2452.453125\n",
            "Epoch Step: 1901 Loss: 3.573016 Tokens per Sec: 2484.610596\n",
            "Epoch Step: 1951 Loss: 2.643897 Tokens per Sec: 2390.632812\n",
            "Epoch Step: 2001 Loss: 3.254049 Tokens per Sec: 2455.572998\n",
            "Epoch Step: 2051 Loss: 2.903275 Tokens per Sec: 2452.402344\n",
            "Epoch Step: 2101 Loss: 2.598036 Tokens per Sec: 2410.685303\n",
            "Epoch Step: 2151 Loss: 3.330843 Tokens per Sec: 2482.051025\n",
            "Epoch Step: 2201 Loss: 2.982486 Tokens per Sec: 2438.992188\n",
            "Epoch Step: 2251 Loss: 2.878551 Tokens per Sec: 2397.295410\n",
            "Epoch Step: 2301 Loss: 3.394460 Tokens per Sec: 2396.953369\n",
            "Epoch Step: 2351 Loss: 3.561213 Tokens per Sec: 2365.246338\n",
            "Epoch Step: 2401 Loss: 2.287982 Tokens per Sec: 2405.357666\n",
            "Epoch Step: 2451 Loss: 2.217519 Tokens per Sec: 2457.047363\n",
            "Epoch Step: 2501 Loss: 3.096203 Tokens per Sec: 2442.558838\n",
            "Epoch Step: 2551 Loss: 2.276827 Tokens per Sec: 2418.171387\n",
            "Epoch Step: 2601 Loss: 2.080455 Tokens per Sec: 2471.693604\n",
            "Epoch Step: 2651 Loss: 2.814586 Tokens per Sec: 2451.135010\n",
            "Epoch Step: 2701 Loss: 2.844206 Tokens per Sec: 2455.402344\n",
            "Epoch Step: 2751 Loss: 2.619894 Tokens per Sec: 2429.936035\n",
            "Epoch Step: 2801 Loss: 3.147935 Tokens per Sec: 2443.703613\n",
            "Epoch Step: 2851 Loss: 3.357157 Tokens per Sec: 2430.638428\n",
            "Epoch Step: 2901 Loss: 3.257058 Tokens per Sec: 2407.338867\n",
            "Epoch Step: 2951 Loss: 2.486779 Tokens per Sec: 2436.483643\n",
            "Epoch Step: 3001 Loss: 3.197849 Tokens per Sec: 2455.732666\n",
            "Epoch Step: 3051 Loss: 3.121497 Tokens per Sec: 2400.504150\n",
            "Epoch Step: 3101 Loss: 2.973531 Tokens per Sec: 2416.789062\n",
            "Epoch Step: 3151 Loss: 2.313656 Tokens per Sec: 2398.407471\n",
            "Epoch Step: 3201 Loss: 3.402284 Tokens per Sec: 2422.908936\n",
            "Epoch Step: 3251 Loss: 3.024850 Tokens per Sec: 2437.066650\n",
            "Epoch Step: 3301 Loss: 2.822996 Tokens per Sec: 2415.895996\n",
            "Epoch Step: 3351 Loss: 3.023468 Tokens per Sec: 2428.563232\n",
            "Epoch Step: 3401 Loss: 3.254328 Tokens per Sec: 2454.030518\n",
            "Epoch Step: 3451 Loss: 2.961222 Tokens per Sec: 2424.898193\n",
            "Epoch Step: 3501 Loss: 3.166468 Tokens per Sec: 2451.643555\n",
            "Epoch Step: 3551 Loss: 2.914861 Tokens per Sec: 2448.723145\n",
            "Epoch Step: 3601 Loss: 3.359723 Tokens per Sec: 2410.996094\n",
            "Epoch Step: 3651 Loss: 2.255657 Tokens per Sec: 2409.511475\n",
            "Epoch Step: 3701 Loss: 3.188419 Tokens per Sec: 2438.062012\n",
            "Epoch Step: 3751 Loss: 3.074796 Tokens per Sec: 2450.543945\n",
            "Epoch Step: 3801 Loss: 2.569551 Tokens per Sec: 2347.022217\n",
            "Epoch Step: 3851 Loss: 2.632272 Tokens per Sec: 2405.170166\n",
            "Epoch Step: 3901 Loss: 3.378225 Tokens per Sec: 2427.573975\n",
            "Epoch Step: 3951 Loss: 3.331712 Tokens per Sec: 2434.275635\n",
            "Epoch Step: 4001 Loss: 2.932558 Tokens per Sec: 2478.650879\n",
            "Epoch Step: 4051 Loss: 2.694922 Tokens per Sec: 2449.771484\n",
            "Epoch Step: 4101 Loss: 3.023707 Tokens per Sec: 2462.861572\n",
            "Epoch Step: 4151 Loss: 3.129614 Tokens per Sec: 2461.589111\n",
            "Epoch Step: 4201 Loss: 3.466693 Tokens per Sec: 2450.171631\n",
            "Epoch Step: 4251 Loss: 2.678611 Tokens per Sec: 2443.218018\n",
            "Epoch Step: 4301 Loss: 2.244877 Tokens per Sec: 2414.738281\n",
            "Epoch Step: 4351 Loss: 3.142484 Tokens per Sec: 2457.372803\n",
            "Epoch Step: 4401 Loss: 2.940821 Tokens per Sec: 2425.946777\n",
            "Epoch Step: 4451 Loss: 2.340739 Tokens per Sec: 2430.762207\n",
            "Epoch Step: 4501 Loss: 2.601664 Tokens per Sec: 2405.852295\n",
            "Epoch Step: 1 Loss: 2.203385 Tokens per Sec: 2081.579346\n",
            "tensor(2.7224, device='cuda:0')\n",
            "Ending epoch 8 ...\n",
            "Starting epoch 9 ...\n",
            "Epoch Step: 1 Loss: 3.053975 Tokens per Sec: 1223.861816\n",
            "Epoch Step: 51 Loss: 3.250502 Tokens per Sec: 2409.804932\n",
            "Epoch Step: 101 Loss: 2.106729 Tokens per Sec: 2459.143799\n",
            "Epoch Step: 151 Loss: 2.713418 Tokens per Sec: 2419.420166\n",
            "Epoch Step: 201 Loss: 2.990672 Tokens per Sec: 2426.055908\n",
            "Epoch Step: 251 Loss: 2.858838 Tokens per Sec: 2421.871094\n",
            "Epoch Step: 301 Loss: 2.993796 Tokens per Sec: 2423.209961\n",
            "Epoch Step: 351 Loss: 3.333484 Tokens per Sec: 2446.331299\n",
            "Epoch Step: 401 Loss: 2.991915 Tokens per Sec: 2466.628906\n",
            "Epoch Step: 451 Loss: 2.513484 Tokens per Sec: 2429.478760\n",
            "Epoch Step: 501 Loss: 3.017539 Tokens per Sec: 2434.975098\n",
            "Epoch Step: 551 Loss: 2.617490 Tokens per Sec: 2463.494629\n",
            "Epoch Step: 601 Loss: 3.367416 Tokens per Sec: 2407.028320\n",
            "Epoch Step: 651 Loss: 3.193753 Tokens per Sec: 2414.052002\n",
            "Epoch Step: 701 Loss: 2.860169 Tokens per Sec: 2474.726074\n",
            "Epoch Step: 751 Loss: 2.943477 Tokens per Sec: 2470.199463\n",
            "Epoch Step: 801 Loss: 3.632924 Tokens per Sec: 2468.671387\n",
            "Epoch Step: 851 Loss: 3.499654 Tokens per Sec: 2408.971191\n",
            "Epoch Step: 901 Loss: 3.091629 Tokens per Sec: 2418.160645\n",
            "Epoch Step: 951 Loss: 3.166929 Tokens per Sec: 2435.211670\n",
            "Epoch Step: 1001 Loss: 3.181404 Tokens per Sec: 2432.671387\n",
            "Epoch Step: 1051 Loss: 3.113107 Tokens per Sec: 2458.047607\n",
            "Epoch Step: 1101 Loss: 2.540202 Tokens per Sec: 2431.940674\n",
            "Epoch Step: 1151 Loss: 3.032087 Tokens per Sec: 2433.431641\n",
            "Epoch Step: 1201 Loss: 3.384203 Tokens per Sec: 2450.685303\n",
            "Epoch Step: 1251 Loss: 2.408007 Tokens per Sec: 2442.278320\n",
            "Epoch Step: 1301 Loss: 1.673884 Tokens per Sec: 2420.210205\n",
            "Epoch Step: 1351 Loss: 3.106030 Tokens per Sec: 2420.153320\n",
            "Epoch Step: 1401 Loss: 2.276694 Tokens per Sec: 2412.422119\n",
            "Epoch Step: 1451 Loss: 3.313121 Tokens per Sec: 2444.523438\n",
            "Epoch Step: 1501 Loss: 2.119182 Tokens per Sec: 2405.211670\n",
            "Epoch Step: 1551 Loss: 3.050591 Tokens per Sec: 2438.918457\n",
            "Epoch Step: 1601 Loss: 3.321613 Tokens per Sec: 2464.266846\n",
            "Epoch Step: 1651 Loss: 3.254249 Tokens per Sec: 2445.660400\n",
            "Epoch Step: 1701 Loss: 0.950234 Tokens per Sec: 2455.094238\n",
            "Epoch Step: 1751 Loss: 2.984440 Tokens per Sec: 2413.198242\n",
            "Epoch Step: 1801 Loss: 2.318796 Tokens per Sec: 2420.034912\n",
            "Epoch Step: 1851 Loss: 2.896427 Tokens per Sec: 2455.481201\n",
            "Epoch Step: 1901 Loss: 3.583242 Tokens per Sec: 2451.627930\n",
            "Epoch Step: 1951 Loss: 2.597242 Tokens per Sec: 2428.023682\n",
            "Epoch Step: 2001 Loss: 3.258025 Tokens per Sec: 2429.961670\n",
            "Epoch Step: 2051 Loss: 2.473717 Tokens per Sec: 2433.039307\n",
            "Epoch Step: 2101 Loss: 2.764513 Tokens per Sec: 2429.545166\n",
            "Epoch Step: 2151 Loss: 3.273768 Tokens per Sec: 2414.407959\n",
            "Epoch Step: 2201 Loss: 3.786761 Tokens per Sec: 2460.880127\n",
            "Epoch Step: 2251 Loss: 3.132836 Tokens per Sec: 2435.876221\n",
            "Epoch Step: 2301 Loss: 2.653688 Tokens per Sec: 2409.591064\n",
            "Epoch Step: 2351 Loss: 3.010777 Tokens per Sec: 2426.964600\n",
            "Epoch Step: 2401 Loss: 3.034483 Tokens per Sec: 2461.002197\n",
            "Epoch Step: 2451 Loss: 2.981938 Tokens per Sec: 2455.555420\n",
            "Epoch Step: 2501 Loss: 2.273446 Tokens per Sec: 2383.377197\n",
            "Epoch Step: 2551 Loss: 3.405463 Tokens per Sec: 2445.657227\n",
            "Epoch Step: 2601 Loss: 3.182206 Tokens per Sec: 2461.673584\n",
            "Epoch Step: 2651 Loss: 2.801324 Tokens per Sec: 2423.287354\n",
            "Epoch Step: 2701 Loss: 3.514124 Tokens per Sec: 2410.333740\n",
            "Epoch Step: 2751 Loss: 2.819603 Tokens per Sec: 2430.321777\n",
            "Epoch Step: 2801 Loss: 3.114781 Tokens per Sec: 2447.102539\n",
            "Epoch Step: 2851 Loss: 2.624046 Tokens per Sec: 2437.467041\n",
            "Epoch Step: 2901 Loss: 2.839721 Tokens per Sec: 2454.348633\n",
            "Epoch Step: 2951 Loss: 3.531068 Tokens per Sec: 2428.639160\n",
            "Epoch Step: 3001 Loss: 3.424174 Tokens per Sec: 2438.369629\n",
            "Epoch Step: 3051 Loss: 2.999781 Tokens per Sec: 2454.122559\n",
            "Epoch Step: 3101 Loss: 3.170904 Tokens per Sec: 2383.241455\n",
            "Epoch Step: 3151 Loss: 2.906551 Tokens per Sec: 2411.278809\n",
            "Epoch Step: 3201 Loss: 3.117854 Tokens per Sec: 2428.100342\n",
            "Epoch Step: 3251 Loss: 2.849030 Tokens per Sec: 2452.289062\n",
            "Epoch Step: 3301 Loss: 2.910515 Tokens per Sec: 2411.211182\n",
            "Epoch Step: 3351 Loss: 3.010416 Tokens per Sec: 2407.483154\n",
            "Epoch Step: 3401 Loss: 2.767133 Tokens per Sec: 2456.507568\n",
            "Epoch Step: 3451 Loss: 2.593960 Tokens per Sec: 2401.504150\n",
            "Epoch Step: 3501 Loss: 3.416807 Tokens per Sec: 2443.907959\n",
            "Epoch Step: 3551 Loss: 3.568826 Tokens per Sec: 2440.201416\n",
            "Epoch Step: 3601 Loss: 2.589920 Tokens per Sec: 2427.155273\n",
            "Epoch Step: 3651 Loss: 3.512154 Tokens per Sec: 2420.717285\n",
            "Epoch Step: 3701 Loss: 3.303925 Tokens per Sec: 2395.470703\n",
            "Epoch Step: 3751 Loss: 2.938743 Tokens per Sec: 2453.892090\n",
            "Epoch Step: 3801 Loss: 2.987431 Tokens per Sec: 2398.870361\n",
            "Epoch Step: 3851 Loss: 2.737427 Tokens per Sec: 2430.280518\n",
            "Epoch Step: 3901 Loss: 3.292007 Tokens per Sec: 2410.546387\n",
            "Epoch Step: 3951 Loss: 3.046539 Tokens per Sec: 2443.624268\n",
            "Epoch Step: 4001 Loss: 3.805784 Tokens per Sec: 2451.430664\n",
            "Epoch Step: 4051 Loss: 3.153052 Tokens per Sec: 2446.239258\n",
            "Epoch Step: 4101 Loss: 2.702900 Tokens per Sec: 2439.518799\n",
            "Epoch Step: 4151 Loss: 3.086050 Tokens per Sec: 2417.795898\n",
            "Epoch Step: 4201 Loss: 3.409344 Tokens per Sec: 2430.268555\n",
            "Epoch Step: 4251 Loss: 2.882704 Tokens per Sec: 2434.561035\n",
            "Epoch Step: 4301 Loss: 3.077035 Tokens per Sec: 2390.200439\n",
            "Epoch Step: 4351 Loss: 3.184211 Tokens per Sec: 2417.628174\n",
            "Epoch Step: 4401 Loss: 3.559930 Tokens per Sec: 2407.230713\n",
            "Epoch Step: 4451 Loss: 3.208417 Tokens per Sec: 2432.896240\n",
            "Epoch Step: 4501 Loss: 2.884889 Tokens per Sec: 2467.382568\n",
            "Epoch Step: 1 Loss: 2.133718 Tokens per Sec: 2078.132080\n",
            "tensor(2.6474, device='cuda:0')\n",
            "Ending epoch 9 ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goCyk93rDeRs",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j9nyvUkMuH",
        "colab_type": "text"
      },
      "source": [
        "**Greedy decoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9-5DWilkNIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayAx1DptjhJe",
        "colab_type": "text"
      },
      "source": [
        "**Evaluating the system**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wko7UUFojhfY",
        "colab_type": "code",
        "outputId": "0bc7581a-c98d-4e38-9833-260282857e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translation:\tIt 's about us . \n",
            "Target:\tIt 's about all of us . \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}